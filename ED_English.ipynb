{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "##Copyright    O., Hamel et S. Lamari for The C00L07UN100120180002 Project.##########################\n",
    "#####################################################################################################\n",
    "\n",
    "#this file contains the code relating to the training of the Bi LSTM paraphrase generation model (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "OMUC7qaMzbC7",
    "outputId": "effa278b-c46e-4f54-92b6-78bdf4536221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C9XdUp1K2_00",
    "outputId": "281328f1-1761-4c4b-e96a-718f4577d0a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ualg-jiEwr3V"
   },
   "outputs": [],
   "source": [
    "#Recuperation of dataset\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "source_path = '/content/drive/My Drive/English DataSet/phrases.txt'\n",
    "target_path = '/content/drive/My Drive/English DataSet/paraprases.txt'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "TPJ6xIFfw292",
    "outputId": "8707439b-f37b-4c64-ccd8-8ddc7dea1551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Brief Stats\n",
      "* number of unique words in input sample sentences: 59237        [this is roughly measured/without any preprocessing]\n",
      "\n",
      "* input sentences\n",
      "\t- number of sentences: 383899\n",
      "\t- avg. number of words in a sentence: 10.795271673018163\n",
      "* output sentences\n",
      "\t- number of sentences: 383897 [data integrity check / should have the same number]\n",
      "\t- avg. number of words in a sentence: 10.896180485911586\n",
      "\n",
      "* Sample sentences range from 0 to 5\n",
      "[1-th] sentence\n",
      "\tinput: what is the step by step guide to invest in share market in india \n",
      "\toutput: what is the step by step guide to invest in share market \n",
      "\n",
      "[2-th] sentence\n",
      "\tinput: what is the story of kohinoor koh - i - noor diamond \n",
      "\toutput: what would happen if the indian government stole the kohinoor koh - i - noor diamond back \n",
      "\n",
      "[3-th] sentence\n",
      "\tinput: how can i increase the speed of my internet connection while using a vpn \n",
      "\toutput: how can internet speed be increased by hacking through dns \n",
      "\n",
      "[4-th] sentence\n",
      "\tinput: why am i mentally very lonely how can i solve it \n",
      "\toutput: find the remainder when math 23 ^ 24 math is divided by 24 23 \n",
      "\n",
      "[5-th] sentence\n",
      "\tinput: which one dissolve in water quikly sugar salt methane and carbon di oxide \n",
      "\toutput: which fish would survive in salt water \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "print('Dataset Brief Stats')\n",
    "print('* number of unique words in input sample sentences: {}\\\n",
    "        [this is roughly measured/without any preprocessing]'.format(len(Counter(source_text.split()))))\n",
    "print()\n",
    "\n",
    "inputSentences = source_text.split('\\n')\n",
    "#inputSentences = texts_1\n",
    "print('* input sentences')\n",
    "print('\\t- number of sentences: {}'.format(len(inputSentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in inputSentences])))\n",
    "\n",
    "outputSentences = target_text.split('\\n')\n",
    "outputSentences = outputSentences[0:-2]\n",
    "#outputSentences = texts_2\n",
    "print('* output sentences')\n",
    "print('\\t- number of sentences: {} [data integrity check / should have the same number]'.format(len(outputSentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in outputSentences])))\n",
    "print()\n",
    "\n",
    "sample_sentence_range = (0, 5)\n",
    "side_by_side_sentences = list(zip(inputSentences, outputSentences))[sample_sentence_range[0]:sample_sentence_range[1]]\n",
    "print('* Sample sentences range from {} to {}'.format(sample_sentence_range[0], sample_sentence_range[1]))\n",
    "\n",
    "for index, sentence in enumerate(side_by_side_sentences):\n",
    "    inp_sent, out_sent = sentence\n",
    "    print('[{}-th] sentence'.format(index+1))\n",
    "    print('\\tinput: {}'.format(inp_sent))\n",
    "    print('\\toutput: {}'.format(out_sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNnm8_gGw91M"
   },
   "outputs": [],
   "source": [
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    vocab = set(text.split())\n",
    "\n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyEn1DoMxCoH"
   },
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ax--0Y1xHzu"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original data (phrase, paraphrase)\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    # to the lower case\n",
    "    \n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "    \n",
    "    # create lookup tables for input and output data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwaNxDnuxNsi"
   },
   "outputs": [],
   "source": [
    "preprocess_and_save_data(source_path, target_path, text_to_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCU85N_lxQfc"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5I296QpxTVe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XvmDROOaxVWA",
    "outputId": "5c6af873-8703-4f5c-e137-95e8620c76a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.2\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SFE48-FxXd1"
   },
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kQG1jKvxaeA"
   },
   "outputs": [],
   "source": [
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uo5DcI_2xcb_"
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gPUprrzxgEv"
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yIQz5NPxjvD"
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojftacWpxoSP"
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1E0fNL_fxrA4"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpegAP-IxuMR"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_30FZKyxyOH"
   },
   "outputs": [],
   "source": [
    "display_step = 100\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "lbnxVMEDx1-P",
    "outputId": "1f8beeb3-9bb8-40e4-fd83-1632231df0fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-15-7302705fbf1e>:11: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-15-7302705fbf1e>:11: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-15-7302705fbf1e>:15: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/seq2seq/python/ops/decoder.py:420: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "save_path = '/content/drive/My Drive/TenWithoutAtt/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-r76dk7x65P"
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LDbsB5-ax-Ho",
    "outputId": "8f79266d-7c0a-49bb-c76c-26410d28120b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  100/2999 - Train Accuracy: 0.6013, Validation Accuracy: 0.6052, Loss: 3.1430\n",
      "Epoch   0 Batch  200/2999 - Train Accuracy: 0.5889, Validation Accuracy: 0.6052, Loss: 3.0879\n",
      "Epoch   0 Batch  300/2999 - Train Accuracy: 0.5938, Validation Accuracy: 0.6052, Loss: 2.9600\n",
      "Epoch   0 Batch  400/2999 - Train Accuracy: 0.5852, Validation Accuracy: 0.6172, Loss: 2.8815\n",
      "Epoch   0 Batch  500/2999 - Train Accuracy: 0.5807, Validation Accuracy: 0.6279, Loss: 2.8416\n",
      "Epoch   0 Batch  600/2999 - Train Accuracy: 0.6583, Validation Accuracy: 0.6341, Loss: 2.2433\n",
      "Epoch   0 Batch  700/2999 - Train Accuracy: 0.5884, Validation Accuracy: 0.6263, Loss: 2.5670\n",
      "Epoch   0 Batch  800/2999 - Train Accuracy: 0.5951, Validation Accuracy: 0.6135, Loss: 2.4598\n",
      "Epoch   0 Batch  900/2999 - Train Accuracy: 0.5304, Validation Accuracy: 0.5690, Loss: 2.4617\n",
      "Epoch   0 Batch 1000/2999 - Train Accuracy: 0.0293, Validation Accuracy: 0.0336, Loss: 2.6452\n",
      "Epoch   0 Batch 1100/2999 - Train Accuracy: 0.0358, Validation Accuracy: 0.0336, Loss: 2.2170\n",
      "Epoch   0 Batch 1200/2999 - Train Accuracy: 0.0615, Validation Accuracy: 0.0888, Loss: 2.2407\n",
      "Epoch   0 Batch 1300/2999 - Train Accuracy: 0.0651, Validation Accuracy: 0.0813, Loss: 2.2901\n",
      "Epoch   0 Batch 1400/2999 - Train Accuracy: 0.0854, Validation Accuracy: 0.1578, Loss: 2.2631\n",
      "Epoch   0 Batch 1500/2999 - Train Accuracy: 0.4706, Validation Accuracy: 0.5161, Loss: 2.3671\n",
      "Epoch   0 Batch 1600/2999 - Train Accuracy: 0.5146, Validation Accuracy: 0.5268, Loss: 2.2472\n",
      "Epoch   0 Batch 1700/2999 - Train Accuracy: 0.5738, Validation Accuracy: 0.5573, Loss: 2.1033\n",
      "Epoch   0 Batch 1800/2999 - Train Accuracy: 0.3647, Validation Accuracy: 0.3245, Loss: 2.3008\n",
      "Epoch   0 Batch 1900/2999 - Train Accuracy: 0.4292, Validation Accuracy: 0.3917, Loss: 2.1613\n",
      "Epoch   0 Batch 2000/2999 - Train Accuracy: 0.3742, Validation Accuracy: 0.3909, Loss: 2.0477\n",
      "Epoch   0 Batch 2100/2999 - Train Accuracy: 0.4399, Validation Accuracy: 0.3685, Loss: 2.1724\n",
      "Epoch   0 Batch 2200/2999 - Train Accuracy: 0.4453, Validation Accuracy: 0.3755, Loss: 2.2943\n",
      "Epoch   0 Batch 2300/2999 - Train Accuracy: 0.4388, Validation Accuracy: 0.3635, Loss: 2.0579\n",
      "Epoch   0 Batch 2400/2999 - Train Accuracy: 0.3987, Validation Accuracy: 0.3690, Loss: 2.3213\n",
      "Epoch   0 Batch 2500/2999 - Train Accuracy: 0.4133, Validation Accuracy: 0.4151, Loss: 2.0796\n",
      "Epoch   0 Batch 2600/2999 - Train Accuracy: 0.4865, Validation Accuracy: 0.4922, Loss: 2.0867\n",
      "Epoch   0 Batch 2700/2999 - Train Accuracy: 0.5962, Validation Accuracy: 0.5729, Loss: 2.0623\n",
      "Epoch   0 Batch 2800/2999 - Train Accuracy: 0.6045, Validation Accuracy: 0.5724, Loss: 1.9983\n",
      "Epoch   0 Batch 2900/2999 - Train Accuracy: 0.6150, Validation Accuracy: 0.6195, Loss: 2.1076\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   1 Batch  100/2999 - Train Accuracy: 0.6120, Validation Accuracy: 0.6112, Loss: 2.0166\n",
      "Epoch   1 Batch  200/2999 - Train Accuracy: 0.5967, Validation Accuracy: 0.6070, Loss: 2.1248\n",
      "Epoch   1 Batch  300/2999 - Train Accuracy: 0.6099, Validation Accuracy: 0.6174, Loss: 2.0668\n",
      "Epoch   1 Batch  400/2999 - Train Accuracy: 0.5927, Validation Accuracy: 0.6172, Loss: 2.1672\n",
      "Epoch   1 Batch  500/2999 - Train Accuracy: 0.5755, Validation Accuracy: 0.6247, Loss: 2.2334\n",
      "Epoch   1 Batch  600/2999 - Train Accuracy: 0.6336, Validation Accuracy: 0.6031, Loss: 1.7928\n",
      "Epoch   1 Batch  700/2999 - Train Accuracy: 0.5854, Validation Accuracy: 0.6156, Loss: 2.0429\n",
      "Epoch   1 Batch  800/2999 - Train Accuracy: 0.5809, Validation Accuracy: 0.6141, Loss: 1.9977\n",
      "Epoch   1 Batch  900/2999 - Train Accuracy: 0.5494, Validation Accuracy: 0.5906, Loss: 2.0387\n",
      "Epoch   1 Batch 1000/2999 - Train Accuracy: 0.5608, Validation Accuracy: 0.5997, Loss: 2.1837\n",
      "Epoch   1 Batch 1100/2999 - Train Accuracy: 0.6164, Validation Accuracy: 0.5971, Loss: 1.8901\n",
      "Epoch   1 Batch 1200/2999 - Train Accuracy: 0.5567, Validation Accuracy: 0.5779, Loss: 1.8588\n",
      "Epoch   1 Batch 1300/2999 - Train Accuracy: 0.5339, Validation Accuracy: 0.5461, Loss: 1.9459\n",
      "Epoch   1 Batch 1400/2999 - Train Accuracy: 0.5547, Validation Accuracy: 0.5815, Loss: 1.8844\n",
      "Epoch   1 Batch 1500/2999 - Train Accuracy: 0.5692, Validation Accuracy: 0.6029, Loss: 2.0594\n",
      "Epoch   1 Batch 1600/2999 - Train Accuracy: 0.5896, Validation Accuracy: 0.5987, Loss: 1.9193\n",
      "Epoch   1 Batch 1700/2999 - Train Accuracy: 0.5956, Validation Accuracy: 0.5701, Loss: 1.8236\n",
      "Epoch   1 Batch 1800/2999 - Train Accuracy: 0.5151, Validation Accuracy: 0.5693, Loss: 2.0105\n",
      "Epoch   1 Batch 1900/2999 - Train Accuracy: 0.5847, Validation Accuracy: 0.5943, Loss: 1.8912\n",
      "Epoch   1 Batch 2000/2999 - Train Accuracy: 0.5797, Validation Accuracy: 0.5859, Loss: 1.7654\n",
      "Epoch   1 Batch 2100/2999 - Train Accuracy: 0.5884, Validation Accuracy: 0.5883, Loss: 1.9090\n",
      "Epoch   1 Batch 2200/2999 - Train Accuracy: 0.5404, Validation Accuracy: 0.5557, Loss: 2.0357\n",
      "Epoch   1 Batch 2300/2999 - Train Accuracy: 0.5997, Validation Accuracy: 0.6076, Loss: 1.8096\n",
      "Epoch   1 Batch 2400/2999 - Train Accuracy: 0.4894, Validation Accuracy: 0.4836, Loss: 2.0666\n",
      "Epoch   1 Batch 2500/2999 - Train Accuracy: 0.5737, Validation Accuracy: 0.5891, Loss: 1.8447\n",
      "Epoch   1 Batch 2600/2999 - Train Accuracy: 0.5911, Validation Accuracy: 0.5687, Loss: 1.8200\n",
      "Epoch   1 Batch 2700/2999 - Train Accuracy: 0.5946, Validation Accuracy: 0.5984, Loss: 1.8571\n",
      "Epoch   1 Batch 2800/2999 - Train Accuracy: 0.5948, Validation Accuracy: 0.5878, Loss: 1.7937\n",
      "Epoch   1 Batch 2900/2999 - Train Accuracy: 0.5865, Validation Accuracy: 0.5992, Loss: 1.9061\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   2 Batch  100/2999 - Train Accuracy: 0.6307, Validation Accuracy: 0.6128, Loss: 1.7824\n",
      "Epoch   2 Batch  200/2999 - Train Accuracy: 0.5671, Validation Accuracy: 0.5977, Loss: 1.9099\n",
      "Epoch   2 Batch  300/2999 - Train Accuracy: 0.5500, Validation Accuracy: 0.5638, Loss: 1.8807\n",
      "Epoch   2 Batch  400/2999 - Train Accuracy: 0.5910, Validation Accuracy: 0.5982, Loss: 2.0092\n",
      "Epoch   2 Batch  500/2999 - Train Accuracy: 0.5720, Validation Accuracy: 0.6195, Loss: 2.0011\n",
      "Epoch   2 Batch  600/2999 - Train Accuracy: 0.6384, Validation Accuracy: 0.6169, Loss: 1.6415\n",
      "Epoch   2 Batch  700/2999 - Train Accuracy: 0.5709, Validation Accuracy: 0.6008, Loss: 1.8550\n",
      "Epoch   2 Batch  800/2999 - Train Accuracy: 0.5714, Validation Accuracy: 0.6039, Loss: 1.8322\n",
      "Epoch   2 Batch  900/2999 - Train Accuracy: 0.5346, Validation Accuracy: 0.5669, Loss: 1.8811\n",
      "Epoch   2 Batch 1000/2999 - Train Accuracy: 0.5614, Validation Accuracy: 0.6201, Loss: 1.9976\n",
      "Epoch   2 Batch 1100/2999 - Train Accuracy: 0.6096, Validation Accuracy: 0.6146, Loss: 1.7047\n",
      "Epoch   2 Batch 1200/2999 - Train Accuracy: 0.6167, Validation Accuracy: 0.5971, Loss: 1.6971\n",
      "Epoch   2 Batch 1300/2999 - Train Accuracy: 0.6047, Validation Accuracy: 0.6081, Loss: 1.8097\n",
      "Epoch   2 Batch 1400/2999 - Train Accuracy: 0.6146, Validation Accuracy: 0.6156, Loss: 1.7388\n",
      "Epoch   2 Batch 1500/2999 - Train Accuracy: 0.5612, Validation Accuracy: 0.5964, Loss: 1.9327\n",
      "Epoch   2 Batch 1600/2999 - Train Accuracy: 0.6193, Validation Accuracy: 0.6240, Loss: 1.7828\n",
      "Epoch   2 Batch 1700/2999 - Train Accuracy: 0.6285, Validation Accuracy: 0.6266, Loss: 1.7076\n",
      "Epoch   2 Batch 1800/2999 - Train Accuracy: 0.6074, Validation Accuracy: 0.6190, Loss: 1.8596\n",
      "Epoch   2 Batch 1900/2999 - Train Accuracy: 0.5985, Validation Accuracy: 0.5961, Loss: 1.7621\n",
      "Epoch   2 Batch 2000/2999 - Train Accuracy: 0.6221, Validation Accuracy: 0.6182, Loss: 1.6429\n",
      "Epoch   2 Batch 2100/2999 - Train Accuracy: 0.6188, Validation Accuracy: 0.6195, Loss: 1.7600\n",
      "Epoch   2 Batch 2200/2999 - Train Accuracy: 0.5905, Validation Accuracy: 0.6096, Loss: 1.8859\n",
      "Epoch   2 Batch 2300/2999 - Train Accuracy: 0.6117, Validation Accuracy: 0.6156, Loss: 1.6599\n",
      "Epoch   2 Batch 2400/2999 - Train Accuracy: 0.5692, Validation Accuracy: 0.6062, Loss: 1.9264\n",
      "Epoch   2 Batch 2500/2999 - Train Accuracy: 0.6070, Validation Accuracy: 0.6227, Loss: 1.7095\n",
      "Epoch   2 Batch 2600/2999 - Train Accuracy: 0.5706, Validation Accuracy: 0.5948, Loss: 1.7064\n",
      "Epoch   2 Batch 2700/2999 - Train Accuracy: 0.5938, Validation Accuracy: 0.6185, Loss: 1.7395\n",
      "Epoch   2 Batch 2800/2999 - Train Accuracy: 0.5973, Validation Accuracy: 0.6073, Loss: 1.6629\n",
      "Epoch   2 Batch 2900/2999 - Train Accuracy: 0.6030, Validation Accuracy: 0.6135, Loss: 1.7869\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   3 Batch  100/2999 - Train Accuracy: 0.6367, Validation Accuracy: 0.6372, Loss: 1.6410\n",
      "Epoch   3 Batch  200/2999 - Train Accuracy: 0.5851, Validation Accuracy: 0.6125, Loss: 1.7935\n",
      "Epoch   3 Batch  300/2999 - Train Accuracy: 0.5747, Validation Accuracy: 0.6141, Loss: 1.7642\n",
      "Epoch   3 Batch  400/2999 - Train Accuracy: 0.6006, Validation Accuracy: 0.6021, Loss: 1.9013\n",
      "Epoch   3 Batch  500/2999 - Train Accuracy: 0.5804, Validation Accuracy: 0.6227, Loss: 1.8590\n",
      "Epoch   3 Batch  600/2999 - Train Accuracy: 0.6522, Validation Accuracy: 0.6250, Loss: 1.5290\n",
      "Epoch   3 Batch  700/2999 - Train Accuracy: 0.5684, Validation Accuracy: 0.6128, Loss: 1.7457\n",
      "Epoch   3 Batch  800/2999 - Train Accuracy: 0.5857, Validation Accuracy: 0.6107, Loss: 1.7106\n",
      "Epoch   3 Batch  900/2999 - Train Accuracy: 0.5982, Validation Accuracy: 0.6247, Loss: 1.7734\n",
      "Epoch   3 Batch 1000/2999 - Train Accuracy: 0.5600, Validation Accuracy: 0.5997, Loss: 1.8906\n",
      "Epoch   3 Batch 1100/2999 - Train Accuracy: 0.6114, Validation Accuracy: 0.6104, Loss: 1.6289\n",
      "Epoch   3 Batch 1200/2999 - Train Accuracy: 0.6038, Validation Accuracy: 0.6122, Loss: 1.6311\n",
      "Epoch   3 Batch 1300/2999 - Train Accuracy: 0.6174, Validation Accuracy: 0.6026, Loss: 1.7266\n",
      "Epoch   3 Batch 1400/2999 - Train Accuracy: 0.6151, Validation Accuracy: 0.6102, Loss: 1.6448\n",
      "Epoch   3 Batch 1500/2999 - Train Accuracy: 0.5577, Validation Accuracy: 0.6029, Loss: 1.8229\n",
      "Epoch   3 Batch 1600/2999 - Train Accuracy: 0.5878, Validation Accuracy: 0.6109, Loss: 1.6996\n",
      "Epoch   3 Batch 1700/2999 - Train Accuracy: 0.6115, Validation Accuracy: 0.6120, Loss: 1.6116\n",
      "Epoch   3 Batch 1800/2999 - Train Accuracy: 0.6099, Validation Accuracy: 0.6242, Loss: 1.7531\n",
      "Epoch   3 Batch 1900/2999 - Train Accuracy: 0.6129, Validation Accuracy: 0.5951, Loss: 1.6727\n",
      "Epoch   3 Batch 2000/2999 - Train Accuracy: 0.6224, Validation Accuracy: 0.5951, Loss: 1.5626\n",
      "Epoch   3 Batch 2100/2999 - Train Accuracy: 0.6096, Validation Accuracy: 0.6156, Loss: 1.6654\n",
      "Epoch   3 Batch 2200/2999 - Train Accuracy: 0.6064, Validation Accuracy: 0.6000, Loss: 1.8043\n",
      "Epoch   3 Batch 2300/2999 - Train Accuracy: 0.6284, Validation Accuracy: 0.6070, Loss: 1.5737\n",
      "Epoch   3 Batch 2400/2999 - Train Accuracy: 0.5611, Validation Accuracy: 0.5951, Loss: 1.8303\n",
      "Epoch   3 Batch 2500/2999 - Train Accuracy: 0.5794, Validation Accuracy: 0.6146, Loss: 1.6132\n",
      "Epoch   3 Batch 2600/2999 - Train Accuracy: 0.5849, Validation Accuracy: 0.5948, Loss: 1.6074\n",
      "Epoch   3 Batch 2700/2999 - Train Accuracy: 0.6021, Validation Accuracy: 0.6130, Loss: 1.6781\n",
      "Epoch   3 Batch 2800/2999 - Train Accuracy: 0.6005, Validation Accuracy: 0.5906, Loss: 1.6103\n",
      "Epoch   3 Batch 2900/2999 - Train Accuracy: 0.5848, Validation Accuracy: 0.6086, Loss: 1.7016\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   4 Batch  100/2999 - Train Accuracy: 0.6268, Validation Accuracy: 0.6133, Loss: 1.5639\n",
      "Epoch   4 Batch  200/2999 - Train Accuracy: 0.5983, Validation Accuracy: 0.6193, Loss: 1.7180\n",
      "Epoch   4 Batch  300/2999 - Train Accuracy: 0.5836, Validation Accuracy: 0.6023, Loss: 1.7029\n",
      "Epoch   4 Batch  400/2999 - Train Accuracy: 0.5917, Validation Accuracy: 0.6023, Loss: 1.8352\n",
      "Epoch   4 Batch  500/2999 - Train Accuracy: 0.5813, Validation Accuracy: 0.6146, Loss: 1.7765\n",
      "Epoch   4 Batch  600/2999 - Train Accuracy: 0.6454, Validation Accuracy: 0.5977, Loss: 1.4741\n",
      "Epoch   4 Batch  700/2999 - Train Accuracy: 0.5660, Validation Accuracy: 0.6094, Loss: 1.6797\n",
      "Epoch   4 Batch  800/2999 - Train Accuracy: 0.5985, Validation Accuracy: 0.6057, Loss: 1.6308\n",
      "Epoch   4 Batch  900/2999 - Train Accuracy: 0.5778, Validation Accuracy: 0.6164, Loss: 1.7236\n",
      "Epoch   4 Batch 1000/2999 - Train Accuracy: 0.5533, Validation Accuracy: 0.5938, Loss: 1.8366\n",
      "Epoch   4 Batch 1100/2999 - Train Accuracy: 0.5852, Validation Accuracy: 0.5935, Loss: 1.5487\n",
      "Epoch   4 Batch 1200/2999 - Train Accuracy: 0.6275, Validation Accuracy: 0.6029, Loss: 1.5520\n",
      "Epoch   4 Batch 1300/2999 - Train Accuracy: 0.6086, Validation Accuracy: 0.6065, Loss: 1.6554\n",
      "Epoch   4 Batch 1400/2999 - Train Accuracy: 0.6109, Validation Accuracy: 0.6164, Loss: 1.5975\n",
      "Epoch   4 Batch 1500/2999 - Train Accuracy: 0.5617, Validation Accuracy: 0.6034, Loss: 1.7615\n",
      "Epoch   4 Batch 1600/2999 - Train Accuracy: 0.5734, Validation Accuracy: 0.5917, Loss: 1.6195\n",
      "Epoch   4 Batch 1700/2999 - Train Accuracy: 0.6129, Validation Accuracy: 0.5948, Loss: 1.5557\n",
      "Epoch   4 Batch 1800/2999 - Train Accuracy: 0.6113, Validation Accuracy: 0.6055, Loss: 1.7084\n",
      "Epoch   4 Batch 1900/2999 - Train Accuracy: 0.5895, Validation Accuracy: 0.5906, Loss: 1.5969\n",
      "Epoch   4 Batch 2000/2999 - Train Accuracy: 0.6174, Validation Accuracy: 0.5875, Loss: 1.4885\n",
      "Epoch   4 Batch 2100/2999 - Train Accuracy: 0.6121, Validation Accuracy: 0.6148, Loss: 1.5953\n",
      "Epoch   4 Batch 2200/2999 - Train Accuracy: 0.6199, Validation Accuracy: 0.5948, Loss: 1.7239\n",
      "Epoch   4 Batch 2300/2999 - Train Accuracy: 0.6273, Validation Accuracy: 0.6128, Loss: 1.5071\n",
      "Epoch   4 Batch 2400/2999 - Train Accuracy: 0.5684, Validation Accuracy: 0.5961, Loss: 1.7811\n",
      "Epoch   4 Batch 2500/2999 - Train Accuracy: 0.6003, Validation Accuracy: 0.5997, Loss: 1.5587\n",
      "Epoch   4 Batch 2600/2999 - Train Accuracy: 0.5824, Validation Accuracy: 0.5945, Loss: 1.5477\n",
      "Epoch   4 Batch 2700/2999 - Train Accuracy: 0.5978, Validation Accuracy: 0.6052, Loss: 1.6246\n",
      "Epoch   4 Batch 2800/2999 - Train Accuracy: 0.5946, Validation Accuracy: 0.6120, Loss: 1.5482\n",
      "Epoch   4 Batch 2900/2999 - Train Accuracy: 0.6027, Validation Accuracy: 0.6185, Loss: 1.6384\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   5 Batch  100/2999 - Train Accuracy: 0.6042, Validation Accuracy: 0.6031, Loss: 1.5200\n",
      "Epoch   5 Batch  200/2999 - Train Accuracy: 0.5964, Validation Accuracy: 0.6154, Loss: 1.6806\n",
      "Epoch   5 Batch  300/2999 - Train Accuracy: 0.6057, Validation Accuracy: 0.6104, Loss: 1.6511\n",
      "Epoch   5 Batch  400/2999 - Train Accuracy: 0.5953, Validation Accuracy: 0.6216, Loss: 1.7797\n",
      "Epoch   5 Batch  500/2999 - Train Accuracy: 0.5865, Validation Accuracy: 0.6237, Loss: 1.7074\n",
      "Epoch   5 Batch  600/2999 - Train Accuracy: 0.6545, Validation Accuracy: 0.6029, Loss: 1.4225\n",
      "Epoch   5 Batch  700/2999 - Train Accuracy: 0.5660, Validation Accuracy: 0.6049, Loss: 1.6134\n",
      "Epoch   5 Batch  800/2999 - Train Accuracy: 0.6035, Validation Accuracy: 0.6073, Loss: 1.5720\n",
      "Epoch   5 Batch  900/2999 - Train Accuracy: 0.5829, Validation Accuracy: 0.6172, Loss: 1.6589\n",
      "Epoch   5 Batch 1000/2999 - Train Accuracy: 0.5809, Validation Accuracy: 0.6135, Loss: 1.7948\n",
      "Epoch   5 Batch 1100/2999 - Train Accuracy: 0.5811, Validation Accuracy: 0.6133, Loss: 1.5245\n",
      "Epoch   5 Batch 1200/2999 - Train Accuracy: 0.6343, Validation Accuracy: 0.6143, Loss: 1.5013\n",
      "Epoch   5 Batch 1300/2999 - Train Accuracy: 0.6167, Validation Accuracy: 0.6250, Loss: 1.6216\n",
      "Epoch   5 Batch 1400/2999 - Train Accuracy: 0.6245, Validation Accuracy: 0.6182, Loss: 1.5287\n",
      "Epoch   5 Batch 1500/2999 - Train Accuracy: 0.5814, Validation Accuracy: 0.6146, Loss: 1.7197\n",
      "Epoch   5 Batch 1600/2999 - Train Accuracy: 0.5924, Validation Accuracy: 0.5919, Loss: 1.5761\n",
      "Epoch   5 Batch 1700/2999 - Train Accuracy: 0.6202, Validation Accuracy: 0.6055, Loss: 1.5148\n",
      "Epoch   5 Batch 1800/2999 - Train Accuracy: 0.6102, Validation Accuracy: 0.6185, Loss: 1.6478\n",
      "Epoch   5 Batch 1900/2999 - Train Accuracy: 0.5990, Validation Accuracy: 0.6023, Loss: 1.5539\n",
      "Epoch   5 Batch 2000/2999 - Train Accuracy: 0.6221, Validation Accuracy: 0.6143, Loss: 1.4440\n",
      "Epoch   5 Batch 2100/2999 - Train Accuracy: 0.6113, Validation Accuracy: 0.6250, Loss: 1.5343\n",
      "Epoch   5 Batch 2200/2999 - Train Accuracy: 0.6064, Validation Accuracy: 0.6146, Loss: 1.7001\n",
      "Epoch   5 Batch 2300/2999 - Train Accuracy: 0.6385, Validation Accuracy: 0.6078, Loss: 1.4647\n",
      "Epoch   5 Batch 2400/2999 - Train Accuracy: 0.5723, Validation Accuracy: 0.6156, Loss: 1.7213\n",
      "Epoch   5 Batch 2500/2999 - Train Accuracy: 0.5917, Validation Accuracy: 0.6078, Loss: 1.5176\n",
      "Epoch   5 Batch 2600/2999 - Train Accuracy: 0.6177, Validation Accuracy: 0.6076, Loss: 1.4769\n",
      "Epoch   5 Batch 2700/2999 - Train Accuracy: 0.6078, Validation Accuracy: 0.6104, Loss: 1.5611\n",
      "Epoch   5 Batch 2800/2999 - Train Accuracy: 0.5973, Validation Accuracy: 0.6026, Loss: 1.5077\n",
      "Epoch   5 Batch 2900/2999 - Train Accuracy: 0.5815, Validation Accuracy: 0.6167, Loss: 1.5876\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   6 Batch  100/2999 - Train Accuracy: 0.6026, Validation Accuracy: 0.6154, Loss: 1.4555\n",
      "Epoch   6 Batch  200/2999 - Train Accuracy: 0.6158, Validation Accuracy: 0.6203, Loss: 1.6315\n",
      "Epoch   6 Batch  300/2999 - Train Accuracy: 0.6013, Validation Accuracy: 0.6120, Loss: 1.5927\n",
      "Epoch   6 Batch  400/2999 - Train Accuracy: 0.6071, Validation Accuracy: 0.6070, Loss: 1.7482\n",
      "Epoch   6 Batch  500/2999 - Train Accuracy: 0.5723, Validation Accuracy: 0.6297, Loss: 1.6506\n",
      "Epoch   6 Batch  600/2999 - Train Accuracy: 0.6595, Validation Accuracy: 0.6193, Loss: 1.3830\n",
      "Epoch   6 Batch  700/2999 - Train Accuracy: 0.5889, Validation Accuracy: 0.6143, Loss: 1.5863\n",
      "Epoch   6 Batch  800/2999 - Train Accuracy: 0.5943, Validation Accuracy: 0.6206, Loss: 1.5278\n",
      "Epoch   6 Batch  900/2999 - Train Accuracy: 0.5826, Validation Accuracy: 0.6104, Loss: 1.6109\n",
      "Epoch   6 Batch 1000/2999 - Train Accuracy: 0.5871, Validation Accuracy: 0.6156, Loss: 1.7690\n",
      "Epoch   6 Batch 1100/2999 - Train Accuracy: 0.6273, Validation Accuracy: 0.6167, Loss: 1.4766\n",
      "Epoch   6 Batch 1200/2999 - Train Accuracy: 0.6431, Validation Accuracy: 0.6268, Loss: 1.4599\n",
      "Epoch   6 Batch 1300/2999 - Train Accuracy: 0.6294, Validation Accuracy: 0.6279, Loss: 1.5768\n",
      "Epoch   6 Batch 1400/2999 - Train Accuracy: 0.6326, Validation Accuracy: 0.6099, Loss: 1.4968\n",
      "Epoch   6 Batch 1500/2999 - Train Accuracy: 0.5841, Validation Accuracy: 0.6255, Loss: 1.6819\n",
      "Epoch   6 Batch 1600/2999 - Train Accuracy: 0.6089, Validation Accuracy: 0.6076, Loss: 1.5517\n",
      "Epoch   6 Batch 1700/2999 - Train Accuracy: 0.6274, Validation Accuracy: 0.6180, Loss: 1.4752\n",
      "Epoch   6 Batch 1800/2999 - Train Accuracy: 0.6060, Validation Accuracy: 0.6164, Loss: 1.6090\n",
      "Epoch   6 Batch 1900/2999 - Train Accuracy: 0.6129, Validation Accuracy: 0.6031, Loss: 1.5389\n",
      "Epoch   6 Batch 2000/2999 - Train Accuracy: 0.6461, Validation Accuracy: 0.6135, Loss: 1.4140\n",
      "Epoch   6 Batch 2100/2999 - Train Accuracy: 0.6298, Validation Accuracy: 0.6359, Loss: 1.5067\n",
      "Epoch   6 Batch 2200/2999 - Train Accuracy: 0.6228, Validation Accuracy: 0.6122, Loss: 1.6556\n",
      "Epoch   6 Batch 2300/2999 - Train Accuracy: 0.6406, Validation Accuracy: 0.6112, Loss: 1.4272\n",
      "Epoch   6 Batch 2400/2999 - Train Accuracy: 0.5957, Validation Accuracy: 0.6125, Loss: 1.6800\n",
      "Epoch   6 Batch 2500/2999 - Train Accuracy: 0.6214, Validation Accuracy: 0.6323, Loss: 1.4925\n",
      "Epoch   6 Batch 2600/2999 - Train Accuracy: 0.6129, Validation Accuracy: 0.6211, Loss: 1.4454\n",
      "Epoch   6 Batch 2700/2999 - Train Accuracy: 0.6126, Validation Accuracy: 0.6206, Loss: 1.5302\n",
      "Epoch   6 Batch 2800/2999 - Train Accuracy: 0.6212, Validation Accuracy: 0.6185, Loss: 1.4664\n",
      "Epoch   6 Batch 2900/2999 - Train Accuracy: 0.6158, Validation Accuracy: 0.6286, Loss: 1.5757\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   7 Batch  100/2999 - Train Accuracy: 0.6398, Validation Accuracy: 0.6060, Loss: 1.4188\n",
      "Epoch   7 Batch  200/2999 - Train Accuracy: 0.6280, Validation Accuracy: 0.6367, Loss: 1.5788\n",
      "Epoch   7 Batch  300/2999 - Train Accuracy: 0.5914, Validation Accuracy: 0.6172, Loss: 1.5449\n",
      "Epoch   7 Batch  400/2999 - Train Accuracy: 0.5983, Validation Accuracy: 0.6138, Loss: 1.6993\n",
      "Epoch   7 Batch  500/2999 - Train Accuracy: 0.5952, Validation Accuracy: 0.6312, Loss: 1.5974\n",
      "Epoch   7 Batch  600/2999 - Train Accuracy: 0.6615, Validation Accuracy: 0.6107, Loss: 1.3542\n",
      "Epoch   7 Batch  700/2999 - Train Accuracy: 0.6072, Validation Accuracy: 0.6323, Loss: 1.5433\n",
      "Epoch   7 Batch  800/2999 - Train Accuracy: 0.5834, Validation Accuracy: 0.6307, Loss: 1.5043\n",
      "Epoch   7 Batch  900/2999 - Train Accuracy: 0.5893, Validation Accuracy: 0.6099, Loss: 1.5752\n",
      "Epoch   7 Batch 1000/2999 - Train Accuracy: 0.5820, Validation Accuracy: 0.6268, Loss: 1.7135\n",
      "Epoch   7 Batch 1100/2999 - Train Accuracy: 0.6119, Validation Accuracy: 0.6073, Loss: 1.4341\n",
      "Epoch   7 Batch 1200/2999 - Train Accuracy: 0.6530, Validation Accuracy: 0.6195, Loss: 1.4291\n",
      "Epoch   7 Batch 1300/2999 - Train Accuracy: 0.6221, Validation Accuracy: 0.6346, Loss: 1.5544\n",
      "Epoch   7 Batch 1400/2999 - Train Accuracy: 0.6258, Validation Accuracy: 0.6177, Loss: 1.4548\n",
      "Epoch   7 Batch 1500/2999 - Train Accuracy: 0.5932, Validation Accuracy: 0.6276, Loss: 1.6591\n",
      "Epoch   7 Batch 1600/2999 - Train Accuracy: 0.6102, Validation Accuracy: 0.6070, Loss: 1.4861\n",
      "Epoch   7 Batch 1700/2999 - Train Accuracy: 0.6312, Validation Accuracy: 0.6227, Loss: 1.4429\n",
      "Epoch   7 Batch 1800/2999 - Train Accuracy: 0.5790, Validation Accuracy: 0.6201, Loss: 1.5535\n",
      "Epoch   7 Batch 1900/2999 - Train Accuracy: 0.6205, Validation Accuracy: 0.6005, Loss: 1.5118\n",
      "Epoch   7 Batch 2000/2999 - Train Accuracy: 0.6349, Validation Accuracy: 0.6188, Loss: 1.3704\n",
      "Epoch   7 Batch 2100/2999 - Train Accuracy: 0.6277, Validation Accuracy: 0.6268, Loss: 1.4728\n",
      "Epoch   7 Batch 2200/2999 - Train Accuracy: 0.6274, Validation Accuracy: 0.6224, Loss: 1.6145\n",
      "Epoch   7 Batch 2300/2999 - Train Accuracy: 0.6372, Validation Accuracy: 0.6180, Loss: 1.3918\n",
      "Epoch   7 Batch 2400/2999 - Train Accuracy: 0.5988, Validation Accuracy: 0.6164, Loss: 1.6438\n",
      "Epoch   7 Batch 2500/2999 - Train Accuracy: 0.6148, Validation Accuracy: 0.6122, Loss: 1.4533\n",
      "Epoch   7 Batch 2600/2999 - Train Accuracy: 0.6215, Validation Accuracy: 0.6164, Loss: 1.4249\n",
      "Epoch   7 Batch 2700/2999 - Train Accuracy: 0.6094, Validation Accuracy: 0.6260, Loss: 1.5127\n",
      "Epoch   7 Batch 2800/2999 - Train Accuracy: 0.6029, Validation Accuracy: 0.6211, Loss: 1.4563\n",
      "Epoch   7 Batch 2900/2999 - Train Accuracy: 0.6094, Validation Accuracy: 0.6302, Loss: 1.5532\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   8 Batch  100/2999 - Train Accuracy: 0.6521, Validation Accuracy: 0.6271, Loss: 1.3820\n",
      "Epoch   8 Batch  200/2999 - Train Accuracy: 0.6008, Validation Accuracy: 0.6354, Loss: 1.5604\n",
      "Epoch   8 Batch  300/2999 - Train Accuracy: 0.6143, Validation Accuracy: 0.6227, Loss: 1.5154\n",
      "Epoch   8 Batch  400/2999 - Train Accuracy: 0.5912, Validation Accuracy: 0.6151, Loss: 1.6931\n",
      "Epoch   8 Batch  500/2999 - Train Accuracy: 0.6042, Validation Accuracy: 0.6346, Loss: 1.5907\n",
      "Epoch   8 Batch  600/2999 - Train Accuracy: 0.6520, Validation Accuracy: 0.6151, Loss: 1.3487\n",
      "Epoch   8 Batch  700/2999 - Train Accuracy: 0.5997, Validation Accuracy: 0.6177, Loss: 1.5195\n",
      "Epoch   8 Batch  800/2999 - Train Accuracy: 0.5963, Validation Accuracy: 0.6370, Loss: 1.4571\n",
      "Epoch   8 Batch  900/2999 - Train Accuracy: 0.5862, Validation Accuracy: 0.6180, Loss: 1.5599\n",
      "Epoch   8 Batch 1000/2999 - Train Accuracy: 0.5924, Validation Accuracy: 0.6305, Loss: 1.6889\n",
      "Epoch   8 Batch 1100/2999 - Train Accuracy: 0.6222, Validation Accuracy: 0.6242, Loss: 1.3967\n",
      "Epoch   8 Batch 1200/2999 - Train Accuracy: 0.6487, Validation Accuracy: 0.6190, Loss: 1.4143\n",
      "Epoch   8 Batch 1300/2999 - Train Accuracy: 0.6310, Validation Accuracy: 0.6372, Loss: 1.5050\n",
      "Epoch   8 Batch 1400/2999 - Train Accuracy: 0.6354, Validation Accuracy: 0.6312, Loss: 1.4306\n",
      "Epoch   8 Batch 1500/2999 - Train Accuracy: 0.5938, Validation Accuracy: 0.6333, Loss: 1.6126\n",
      "Epoch   8 Batch 1600/2999 - Train Accuracy: 0.6320, Validation Accuracy: 0.6318, Loss: 1.4778\n",
      "Epoch   8 Batch 1700/2999 - Train Accuracy: 0.6288, Validation Accuracy: 0.6180, Loss: 1.4025\n",
      "Epoch   8 Batch 1800/2999 - Train Accuracy: 0.6013, Validation Accuracy: 0.6326, Loss: 1.5293\n",
      "Epoch   8 Batch 1900/2999 - Train Accuracy: 0.6139, Validation Accuracy: 0.6156, Loss: 1.4773\n",
      "Epoch   8 Batch 2000/2999 - Train Accuracy: 0.6383, Validation Accuracy: 0.6201, Loss: 1.3541\n",
      "Epoch   8 Batch 2100/2999 - Train Accuracy: 0.6309, Validation Accuracy: 0.6396, Loss: 1.4257\n",
      "Epoch   8 Batch 2200/2999 - Train Accuracy: 0.6191, Validation Accuracy: 0.6260, Loss: 1.5832\n",
      "Epoch   8 Batch 2300/2999 - Train Accuracy: 0.6279, Validation Accuracy: 0.6201, Loss: 1.3575\n",
      "Epoch   8 Batch 2400/2999 - Train Accuracy: 0.5971, Validation Accuracy: 0.6333, Loss: 1.6186\n",
      "Epoch   8 Batch 2500/2999 - Train Accuracy: 0.6258, Validation Accuracy: 0.6229, Loss: 1.4368\n",
      "Epoch   8 Batch 2600/2999 - Train Accuracy: 0.6234, Validation Accuracy: 0.6182, Loss: 1.3790\n",
      "Epoch   8 Batch 2700/2999 - Train Accuracy: 0.6150, Validation Accuracy: 0.6258, Loss: 1.4712\n",
      "Epoch   8 Batch 2800/2999 - Train Accuracy: 0.6064, Validation Accuracy: 0.6159, Loss: 1.4071\n",
      "Epoch   8 Batch 2900/2999 - Train Accuracy: 0.6275, Validation Accuracy: 0.6383, Loss: 1.5123\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch   9 Batch  100/2999 - Train Accuracy: 0.6534, Validation Accuracy: 0.6276, Loss: 1.3630\n",
      "Epoch   9 Batch  200/2999 - Train Accuracy: 0.6164, Validation Accuracy: 0.6398, Loss: 1.5360\n",
      "Epoch   9 Batch  300/2999 - Train Accuracy: 0.6133, Validation Accuracy: 0.6323, Loss: 1.4917\n",
      "Epoch   9 Batch  400/2999 - Train Accuracy: 0.6061, Validation Accuracy: 0.6078, Loss: 1.6492\n",
      "Epoch   9 Batch  500/2999 - Train Accuracy: 0.6019, Validation Accuracy: 0.6227, Loss: 1.5421\n",
      "Epoch   9 Batch  600/2999 - Train Accuracy: 0.6608, Validation Accuracy: 0.6224, Loss: 1.3013\n",
      "Epoch   9 Batch  700/2999 - Train Accuracy: 0.6008, Validation Accuracy: 0.6141, Loss: 1.4923\n",
      "Epoch   9 Batch  800/2999 - Train Accuracy: 0.5991, Validation Accuracy: 0.6247, Loss: 1.4266\n",
      "Epoch   9 Batch  900/2999 - Train Accuracy: 0.5993, Validation Accuracy: 0.6109, Loss: 1.5273\n",
      "Epoch   9 Batch 1000/2999 - Train Accuracy: 0.5946, Validation Accuracy: 0.6240, Loss: 1.6569\n",
      "Epoch   9 Batch 1100/2999 - Train Accuracy: 0.6210, Validation Accuracy: 0.6273, Loss: 1.3855\n",
      "Epoch   9 Batch 1200/2999 - Train Accuracy: 0.6641, Validation Accuracy: 0.6284, Loss: 1.3702\n",
      "Epoch   9 Batch 1300/2999 - Train Accuracy: 0.6370, Validation Accuracy: 0.6245, Loss: 1.4900\n",
      "Epoch   9 Batch 1400/2999 - Train Accuracy: 0.6427, Validation Accuracy: 0.6271, Loss: 1.4116\n",
      "Epoch   9 Batch 1500/2999 - Train Accuracy: 0.5981, Validation Accuracy: 0.6315, Loss: 1.5962\n",
      "Epoch   9 Batch 1600/2999 - Train Accuracy: 0.6273, Validation Accuracy: 0.6365, Loss: 1.4359\n",
      "Epoch   9 Batch 1700/2999 - Train Accuracy: 0.6474, Validation Accuracy: 0.6240, Loss: 1.3745\n",
      "Epoch   9 Batch 1800/2999 - Train Accuracy: 0.5815, Validation Accuracy: 0.6128, Loss: 1.5254\n",
      "Epoch   9 Batch 1900/2999 - Train Accuracy: 0.6195, Validation Accuracy: 0.6271, Loss: 1.4537\n",
      "Epoch   9 Batch 2000/2999 - Train Accuracy: 0.6385, Validation Accuracy: 0.6331, Loss: 1.3414\n",
      "Epoch   9 Batch 2100/2999 - Train Accuracy: 0.6379, Validation Accuracy: 0.6326, Loss: 1.3968\n",
      "Epoch   9 Batch 2200/2999 - Train Accuracy: 0.6266, Validation Accuracy: 0.6367, Loss: 1.5618\n",
      "Epoch   9 Batch 2300/2999 - Train Accuracy: 0.6500, Validation Accuracy: 0.6266, Loss: 1.3446\n",
      "Epoch   9 Batch 2400/2999 - Train Accuracy: 0.6150, Validation Accuracy: 0.6341, Loss: 1.6004\n",
      "Epoch   9 Batch 2500/2999 - Train Accuracy: 0.6146, Validation Accuracy: 0.6180, Loss: 1.4122\n",
      "Epoch   9 Batch 2600/2999 - Train Accuracy: 0.6269, Validation Accuracy: 0.6224, Loss: 1.3697\n",
      "Epoch   9 Batch 2700/2999 - Train Accuracy: 0.6193, Validation Accuracy: 0.6367, Loss: 1.4554\n",
      "Epoch   9 Batch 2800/2999 - Train Accuracy: 0.6280, Validation Accuracy: 0.6237, Loss: 1.3858\n",
      "Epoch   9 Batch 2900/2999 - Train Accuracy: 0.6200, Validation Accuracy: 0.6141, Loss: 1.4785\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  10 Batch  100/2999 - Train Accuracy: 0.6458, Validation Accuracy: 0.6185, Loss: 1.3321\n",
      "Epoch  10 Batch  200/2999 - Train Accuracy: 0.6191, Validation Accuracy: 0.6289, Loss: 1.4845\n",
      "Epoch  10 Batch  300/2999 - Train Accuracy: 0.6177, Validation Accuracy: 0.6284, Loss: 1.4631\n",
      "Epoch  10 Batch  400/2999 - Train Accuracy: 0.6006, Validation Accuracy: 0.6253, Loss: 1.6271\n",
      "Epoch  10 Batch  500/2999 - Train Accuracy: 0.5990, Validation Accuracy: 0.6346, Loss: 1.5099\n",
      "Epoch  10 Batch  600/2999 - Train Accuracy: 0.6535, Validation Accuracy: 0.6299, Loss: 1.3029\n",
      "Epoch  10 Batch  700/2999 - Train Accuracy: 0.6083, Validation Accuracy: 0.6250, Loss: 1.4605\n",
      "Epoch  10 Batch  800/2999 - Train Accuracy: 0.6097, Validation Accuracy: 0.6172, Loss: 1.4028\n",
      "Epoch  10 Batch  900/2999 - Train Accuracy: 0.5873, Validation Accuracy: 0.6216, Loss: 1.4884\n",
      "Epoch  10 Batch 1000/2999 - Train Accuracy: 0.5935, Validation Accuracy: 0.6323, Loss: 1.6476\n",
      "Epoch  10 Batch 1100/2999 - Train Accuracy: 0.6343, Validation Accuracy: 0.6195, Loss: 1.3631\n",
      "Epoch  10 Batch 1200/2999 - Train Accuracy: 0.6706, Validation Accuracy: 0.6250, Loss: 1.3598\n",
      "Epoch  10 Batch 1300/2999 - Train Accuracy: 0.6159, Validation Accuracy: 0.6344, Loss: 1.4586\n",
      "Epoch  10 Batch 1400/2999 - Train Accuracy: 0.6346, Validation Accuracy: 0.6115, Loss: 1.4018\n",
      "Epoch  10 Batch 1500/2999 - Train Accuracy: 0.6091, Validation Accuracy: 0.6354, Loss: 1.5418\n",
      "Epoch  10 Batch 1600/2999 - Train Accuracy: 0.6336, Validation Accuracy: 0.6320, Loss: 1.4230\n",
      "Epoch  10 Batch 1700/2999 - Train Accuracy: 0.6414, Validation Accuracy: 0.6284, Loss: 1.3630\n",
      "Epoch  10 Batch 1800/2999 - Train Accuracy: 0.5912, Validation Accuracy: 0.6091, Loss: 1.4734\n",
      "Epoch  10 Batch 1900/2999 - Train Accuracy: 0.6197, Validation Accuracy: 0.6372, Loss: 1.4274\n",
      "Epoch  10 Batch 2000/2999 - Train Accuracy: 0.6326, Validation Accuracy: 0.6247, Loss: 1.3176\n",
      "Epoch  10 Batch 2100/2999 - Train Accuracy: 0.6355, Validation Accuracy: 0.6279, Loss: 1.3804\n",
      "Epoch  10 Batch 2200/2999 - Train Accuracy: 0.6166, Validation Accuracy: 0.6279, Loss: 1.5303\n",
      "Epoch  10 Batch 2300/2999 - Train Accuracy: 0.6471, Validation Accuracy: 0.6159, Loss: 1.3163\n",
      "Epoch  10 Batch 2400/2999 - Train Accuracy: 0.6071, Validation Accuracy: 0.6299, Loss: 1.5336\n",
      "Epoch  10 Batch 2500/2999 - Train Accuracy: 0.6253, Validation Accuracy: 0.6227, Loss: 1.3823\n",
      "Epoch  10 Batch 2600/2999 - Train Accuracy: 0.6377, Validation Accuracy: 0.6104, Loss: 1.3348\n",
      "Epoch  10 Batch 2700/2999 - Train Accuracy: 0.5964, Validation Accuracy: 0.6122, Loss: 1.4194\n",
      "Epoch  10 Batch 2800/2999 - Train Accuracy: 0.6169, Validation Accuracy: 0.6172, Loss: 1.3649\n",
      "Epoch  10 Batch 2900/2999 - Train Accuracy: 0.6239, Validation Accuracy: 0.6159, Loss: 1.4580\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  11 Batch  100/2999 - Train Accuracy: 0.6622, Validation Accuracy: 0.6320, Loss: 1.3215\n",
      "Epoch  11 Batch  200/2999 - Train Accuracy: 0.6148, Validation Accuracy: 0.6292, Loss: 1.4993\n",
      "Epoch  11 Batch  300/2999 - Train Accuracy: 0.6172, Validation Accuracy: 0.6255, Loss: 1.4540\n",
      "Epoch  11 Batch  400/2999 - Train Accuracy: 0.6018, Validation Accuracy: 0.6227, Loss: 1.6131\n",
      "Epoch  11 Batch  500/2999 - Train Accuracy: 0.5935, Validation Accuracy: 0.6128, Loss: 1.4676\n",
      "Epoch  11 Batch  600/2999 - Train Accuracy: 0.6535, Validation Accuracy: 0.6161, Loss: 1.2733\n",
      "Epoch  11 Batch  700/2999 - Train Accuracy: 0.6134, Validation Accuracy: 0.6122, Loss: 1.4694\n",
      "Epoch  11 Batch  800/2999 - Train Accuracy: 0.6147, Validation Accuracy: 0.6177, Loss: 1.4008\n",
      "Epoch  11 Batch  900/2999 - Train Accuracy: 0.5946, Validation Accuracy: 0.6128, Loss: 1.5098\n",
      "Epoch  11 Batch 1000/2999 - Train Accuracy: 0.5940, Validation Accuracy: 0.6286, Loss: 1.6288\n",
      "Epoch  11 Batch 1100/2999 - Train Accuracy: 0.6245, Validation Accuracy: 0.6177, Loss: 1.3287\n",
      "Epoch  11 Batch 1200/2999 - Train Accuracy: 0.6610, Validation Accuracy: 0.6180, Loss: 1.3329\n",
      "Epoch  11 Batch 1300/2999 - Train Accuracy: 0.6359, Validation Accuracy: 0.6198, Loss: 1.4550\n",
      "Epoch  11 Batch 1400/2999 - Train Accuracy: 0.6357, Validation Accuracy: 0.6180, Loss: 1.3624\n",
      "Epoch  11 Batch 1500/2999 - Train Accuracy: 0.6153, Validation Accuracy: 0.6367, Loss: 1.5512\n",
      "Epoch  11 Batch 1600/2999 - Train Accuracy: 0.6310, Validation Accuracy: 0.6344, Loss: 1.3950\n",
      "Epoch  11 Batch 1700/2999 - Train Accuracy: 0.6463, Validation Accuracy: 0.6331, Loss: 1.3450\n",
      "Epoch  11 Batch 1800/2999 - Train Accuracy: 0.5985, Validation Accuracy: 0.6000, Loss: 1.4576\n",
      "Epoch  11 Batch 1900/2999 - Train Accuracy: 0.6159, Validation Accuracy: 0.6320, Loss: 1.4219\n",
      "Epoch  11 Batch 2000/2999 - Train Accuracy: 0.6391, Validation Accuracy: 0.6120, Loss: 1.3044\n",
      "Epoch  11 Batch 2100/2999 - Train Accuracy: 0.6350, Validation Accuracy: 0.6281, Loss: 1.3626\n",
      "Epoch  11 Batch 2200/2999 - Train Accuracy: 0.6218, Validation Accuracy: 0.6299, Loss: 1.5186\n",
      "Epoch  11 Batch 2300/2999 - Train Accuracy: 0.6320, Validation Accuracy: 0.6349, Loss: 1.2925\n",
      "Epoch  11 Batch 2400/2999 - Train Accuracy: 0.5943, Validation Accuracy: 0.6380, Loss: 1.5360\n",
      "Epoch  11 Batch 2500/2999 - Train Accuracy: 0.6391, Validation Accuracy: 0.6154, Loss: 1.3599\n",
      "Epoch  11 Batch 2600/2999 - Train Accuracy: 0.6425, Validation Accuracy: 0.6320, Loss: 1.3299\n",
      "Epoch  11 Batch 2700/2999 - Train Accuracy: 0.6193, Validation Accuracy: 0.6385, Loss: 1.4039\n",
      "Epoch  11 Batch 2800/2999 - Train Accuracy: 0.6196, Validation Accuracy: 0.6201, Loss: 1.3717\n",
      "Epoch  11 Batch 2900/2999 - Train Accuracy: 0.6270, Validation Accuracy: 0.6312, Loss: 1.4347\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  12 Batch  100/2999 - Train Accuracy: 0.6542, Validation Accuracy: 0.6271, Loss: 1.2776\n",
      "Epoch  12 Batch  200/2999 - Train Accuracy: 0.6056, Validation Accuracy: 0.6154, Loss: 1.4769\n",
      "Epoch  12 Batch  300/2999 - Train Accuracy: 0.6266, Validation Accuracy: 0.6276, Loss: 1.4122\n",
      "Epoch  12 Batch  400/2999 - Train Accuracy: 0.5920, Validation Accuracy: 0.6260, Loss: 1.6029\n",
      "Epoch  12 Batch  500/2999 - Train Accuracy: 0.5961, Validation Accuracy: 0.6224, Loss: 1.4566\n",
      "Epoch  12 Batch  600/2999 - Train Accuracy: 0.6525, Validation Accuracy: 0.6161, Loss: 1.2458\n",
      "Epoch  12 Batch  700/2999 - Train Accuracy: 0.6002, Validation Accuracy: 0.6193, Loss: 1.4340\n",
      "Epoch  12 Batch  800/2999 - Train Accuracy: 0.6172, Validation Accuracy: 0.6125, Loss: 1.3592\n",
      "Epoch  12 Batch  900/2999 - Train Accuracy: 0.6083, Validation Accuracy: 0.6271, Loss: 1.4647\n",
      "Epoch  12 Batch 1000/2999 - Train Accuracy: 0.5982, Validation Accuracy: 0.6198, Loss: 1.6013\n",
      "Epoch  12 Batch 1100/2999 - Train Accuracy: 0.6373, Validation Accuracy: 0.6305, Loss: 1.3360\n",
      "Epoch  12 Batch 1200/2999 - Train Accuracy: 0.6527, Validation Accuracy: 0.6323, Loss: 1.3169\n",
      "Epoch  12 Batch 1300/2999 - Train Accuracy: 0.6411, Validation Accuracy: 0.6375, Loss: 1.4218\n",
      "Epoch  12 Batch 1400/2999 - Train Accuracy: 0.6279, Validation Accuracy: 0.6201, Loss: 1.3653\n",
      "Epoch  12 Batch 1500/2999 - Train Accuracy: 0.6142, Validation Accuracy: 0.6307, Loss: 1.5235\n",
      "Epoch  12 Batch 1600/2999 - Train Accuracy: 0.6315, Validation Accuracy: 0.6292, Loss: 1.3749\n",
      "Epoch  12 Batch 1700/2999 - Train Accuracy: 0.6350, Validation Accuracy: 0.6279, Loss: 1.3164\n",
      "Epoch  12 Batch 1800/2999 - Train Accuracy: 0.6057, Validation Accuracy: 0.6221, Loss: 1.4564\n",
      "Epoch  12 Batch 1900/2999 - Train Accuracy: 0.6169, Validation Accuracy: 0.6297, Loss: 1.4006\n",
      "Epoch  12 Batch 2000/2999 - Train Accuracy: 0.6375, Validation Accuracy: 0.6328, Loss: 1.2723\n",
      "Epoch  12 Batch 2100/2999 - Train Accuracy: 0.6457, Validation Accuracy: 0.6276, Loss: 1.3395\n",
      "Epoch  12 Batch 2200/2999 - Train Accuracy: 0.6223, Validation Accuracy: 0.6341, Loss: 1.4933\n",
      "Epoch  12 Batch 2300/2999 - Train Accuracy: 0.6318, Validation Accuracy: 0.6148, Loss: 1.2842\n",
      "Epoch  12 Batch 2400/2999 - Train Accuracy: 0.5968, Validation Accuracy: 0.6138, Loss: 1.5083\n",
      "Epoch  12 Batch 2500/2999 - Train Accuracy: 0.6404, Validation Accuracy: 0.6193, Loss: 1.3449\n",
      "Epoch  12 Batch 2600/2999 - Train Accuracy: 0.6358, Validation Accuracy: 0.6294, Loss: 1.3185\n",
      "Epoch  12 Batch 2700/2999 - Train Accuracy: 0.6226, Validation Accuracy: 0.6312, Loss: 1.4080\n",
      "Epoch  12 Batch 2800/2999 - Train Accuracy: 0.6196, Validation Accuracy: 0.6260, Loss: 1.3315\n",
      "Epoch  12 Batch 2900/2999 - Train Accuracy: 0.6208, Validation Accuracy: 0.6375, Loss: 1.4296\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  13 Batch  100/2999 - Train Accuracy: 0.6586, Validation Accuracy: 0.6258, Loss: 1.2562\n",
      "Epoch  13 Batch  200/2999 - Train Accuracy: 0.6040, Validation Accuracy: 0.6341, Loss: 1.4316\n",
      "Epoch  13 Batch  300/2999 - Train Accuracy: 0.6250, Validation Accuracy: 0.6411, Loss: 1.4091\n",
      "Epoch  13 Batch  400/2999 - Train Accuracy: 0.6061, Validation Accuracy: 0.6279, Loss: 1.5885\n",
      "Epoch  13 Batch  500/2999 - Train Accuracy: 0.6091, Validation Accuracy: 0.6258, Loss: 1.4330\n",
      "Epoch  13 Batch  600/2999 - Train Accuracy: 0.6542, Validation Accuracy: 0.6104, Loss: 1.2381\n",
      "Epoch  13 Batch  700/2999 - Train Accuracy: 0.6113, Validation Accuracy: 0.6328, Loss: 1.4059\n",
      "Epoch  13 Batch  800/2999 - Train Accuracy: 0.6297, Validation Accuracy: 0.6302, Loss: 1.3698\n",
      "Epoch  13 Batch  900/2999 - Train Accuracy: 0.5982, Validation Accuracy: 0.6357, Loss: 1.4521\n",
      "Epoch  13 Batch 1000/2999 - Train Accuracy: 0.6097, Validation Accuracy: 0.6281, Loss: 1.6098\n",
      "Epoch  13 Batch 1100/2999 - Train Accuracy: 0.6321, Validation Accuracy: 0.6242, Loss: 1.3117\n",
      "Epoch  13 Batch 1200/2999 - Train Accuracy: 0.6618, Validation Accuracy: 0.6260, Loss: 1.2855\n",
      "Epoch  13 Batch 1300/2999 - Train Accuracy: 0.6404, Validation Accuracy: 0.6365, Loss: 1.4042\n",
      "Epoch  13 Batch 1400/2999 - Train Accuracy: 0.6242, Validation Accuracy: 0.6151, Loss: 1.3374\n",
      "Epoch  13 Batch 1500/2999 - Train Accuracy: 0.6096, Validation Accuracy: 0.6393, Loss: 1.5141\n",
      "Epoch  13 Batch 1600/2999 - Train Accuracy: 0.6279, Validation Accuracy: 0.6341, Loss: 1.3573\n",
      "Epoch  13 Batch 1700/2999 - Train Accuracy: 0.6441, Validation Accuracy: 0.6224, Loss: 1.3128\n",
      "Epoch  13 Batch 1800/2999 - Train Accuracy: 0.6099, Validation Accuracy: 0.6268, Loss: 1.4277\n",
      "Epoch  13 Batch 1900/2999 - Train Accuracy: 0.6144, Validation Accuracy: 0.6185, Loss: 1.3766\n",
      "Epoch  13 Batch 2000/2999 - Train Accuracy: 0.6310, Validation Accuracy: 0.6271, Loss: 1.2661\n",
      "Epoch  13 Batch 2100/2999 - Train Accuracy: 0.6422, Validation Accuracy: 0.6247, Loss: 1.3406\n",
      "Epoch  13 Batch 2200/2999 - Train Accuracy: 0.6239, Validation Accuracy: 0.6385, Loss: 1.4686\n",
      "Epoch  13 Batch 2300/2999 - Train Accuracy: 0.6375, Validation Accuracy: 0.6211, Loss: 1.2584\n",
      "Epoch  13 Batch 2400/2999 - Train Accuracy: 0.5915, Validation Accuracy: 0.6307, Loss: 1.4952\n",
      "Epoch  13 Batch 2500/2999 - Train Accuracy: 0.6398, Validation Accuracy: 0.6201, Loss: 1.3233\n",
      "Epoch  13 Batch 2600/2999 - Train Accuracy: 0.6414, Validation Accuracy: 0.6284, Loss: 1.3077\n",
      "Epoch  13 Batch 2700/2999 - Train Accuracy: 0.6188, Validation Accuracy: 0.6211, Loss: 1.3784\n",
      "Epoch  13 Batch 2800/2999 - Train Accuracy: 0.6175, Validation Accuracy: 0.6029, Loss: 1.3200\n",
      "Epoch  13 Batch 2900/2999 - Train Accuracy: 0.6348, Validation Accuracy: 0.6216, Loss: 1.3948\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  14 Batch  100/2999 - Train Accuracy: 0.6641, Validation Accuracy: 0.6318, Loss: 1.2596\n",
      "Epoch  14 Batch  200/2999 - Train Accuracy: 0.6099, Validation Accuracy: 0.6221, Loss: 1.4080\n",
      "Epoch  14 Batch  300/2999 - Train Accuracy: 0.6346, Validation Accuracy: 0.6260, Loss: 1.3976\n",
      "Epoch  14 Batch  400/2999 - Train Accuracy: 0.6003, Validation Accuracy: 0.6253, Loss: 1.5673\n",
      "Epoch  14 Batch  500/2999 - Train Accuracy: 0.6172, Validation Accuracy: 0.6292, Loss: 1.4064\n",
      "Epoch  14 Batch  600/2999 - Train Accuracy: 0.6547, Validation Accuracy: 0.6276, Loss: 1.2350\n",
      "Epoch  14 Batch  700/2999 - Train Accuracy: 0.6131, Validation Accuracy: 0.6214, Loss: 1.3955\n",
      "Epoch  14 Batch  800/2999 - Train Accuracy: 0.6214, Validation Accuracy: 0.6271, Loss: 1.3480\n",
      "Epoch  14 Batch  900/2999 - Train Accuracy: 0.5982, Validation Accuracy: 0.6237, Loss: 1.4418\n",
      "Epoch  14 Batch 1000/2999 - Train Accuracy: 0.6091, Validation Accuracy: 0.6318, Loss: 1.5756\n",
      "Epoch  14 Batch 1100/2999 - Train Accuracy: 0.6263, Validation Accuracy: 0.6146, Loss: 1.2934\n",
      "Epoch  14 Batch 1200/2999 - Train Accuracy: 0.6525, Validation Accuracy: 0.6203, Loss: 1.2815\n",
      "Epoch  14 Batch 1300/2999 - Train Accuracy: 0.6458, Validation Accuracy: 0.6307, Loss: 1.4011\n",
      "Epoch  14 Batch 1400/2999 - Train Accuracy: 0.6320, Validation Accuracy: 0.6255, Loss: 1.3226\n",
      "Epoch  14 Batch 1500/2999 - Train Accuracy: 0.6169, Validation Accuracy: 0.6292, Loss: 1.4813\n",
      "Epoch  14 Batch 1600/2999 - Train Accuracy: 0.6370, Validation Accuracy: 0.6135, Loss: 1.3358\n",
      "Epoch  14 Batch 1700/2999 - Train Accuracy: 0.6576, Validation Accuracy: 0.6346, Loss: 1.2825\n",
      "Epoch  14 Batch 1800/2999 - Train Accuracy: 0.6113, Validation Accuracy: 0.6276, Loss: 1.4165\n",
      "Epoch  14 Batch 1900/2999 - Train Accuracy: 0.6220, Validation Accuracy: 0.6326, Loss: 1.3684\n",
      "Epoch  14 Batch 2000/2999 - Train Accuracy: 0.6404, Validation Accuracy: 0.6346, Loss: 1.2561\n",
      "Epoch  14 Batch 2100/2999 - Train Accuracy: 0.6457, Validation Accuracy: 0.6367, Loss: 1.3075\n",
      "Epoch  14 Batch 2200/2999 - Train Accuracy: 0.6193, Validation Accuracy: 0.6357, Loss: 1.4659\n",
      "Epoch  14 Batch 2300/2999 - Train Accuracy: 0.6404, Validation Accuracy: 0.6294, Loss: 1.2340\n",
      "Epoch  14 Batch 2400/2999 - Train Accuracy: 0.5988, Validation Accuracy: 0.6289, Loss: 1.4869\n",
      "Epoch  14 Batch 2500/2999 - Train Accuracy: 0.6404, Validation Accuracy: 0.6227, Loss: 1.3147\n",
      "Epoch  14 Batch 2600/2999 - Train Accuracy: 0.6479, Validation Accuracy: 0.6227, Loss: 1.2892\n",
      "Epoch  14 Batch 2700/2999 - Train Accuracy: 0.6166, Validation Accuracy: 0.6219, Loss: 1.3564\n",
      "Epoch  14 Batch 2800/2999 - Train Accuracy: 0.6115, Validation Accuracy: 0.6138, Loss: 1.3131\n",
      "Epoch  14 Batch 2900/2999 - Train Accuracy: 0.6359, Validation Accuracy: 0.6266, Loss: 1.3997\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  15 Batch  100/2999 - Train Accuracy: 0.6781, Validation Accuracy: 0.6427, Loss: 1.2453\n",
      "Epoch  15 Batch  200/2999 - Train Accuracy: 0.6210, Validation Accuracy: 0.6385, Loss: 1.4157\n",
      "Epoch  15 Batch  300/2999 - Train Accuracy: 0.6182, Validation Accuracy: 0.6286, Loss: 1.3780\n",
      "Epoch  15 Batch  400/2999 - Train Accuracy: 0.6076, Validation Accuracy: 0.6182, Loss: 1.5532\n",
      "Epoch  15 Batch  500/2999 - Train Accuracy: 0.6160, Validation Accuracy: 0.6346, Loss: 1.3963\n",
      "Epoch  15 Batch  600/2999 - Train Accuracy: 0.6583, Validation Accuracy: 0.6141, Loss: 1.2201\n",
      "Epoch  15 Batch  700/2999 - Train Accuracy: 0.6123, Validation Accuracy: 0.6245, Loss: 1.3975\n",
      "Epoch  15 Batch  800/2999 - Train Accuracy: 0.6345, Validation Accuracy: 0.6362, Loss: 1.3115\n",
      "Epoch  15 Batch  900/2999 - Train Accuracy: 0.6027, Validation Accuracy: 0.6258, Loss: 1.4078\n",
      "Epoch  15 Batch 1000/2999 - Train Accuracy: 0.6127, Validation Accuracy: 0.6383, Loss: 1.5515\n",
      "Epoch  15 Batch 1100/2999 - Train Accuracy: 0.6530, Validation Accuracy: 0.6352, Loss: 1.2832\n",
      "Epoch  15 Batch 1200/2999 - Train Accuracy: 0.6741, Validation Accuracy: 0.6292, Loss: 1.2731\n",
      "Epoch  15 Batch 1300/2999 - Train Accuracy: 0.6378, Validation Accuracy: 0.6445, Loss: 1.3779\n",
      "Epoch  15 Batch 1400/2999 - Train Accuracy: 0.6250, Validation Accuracy: 0.6214, Loss: 1.3207\n",
      "Epoch  15 Batch 1500/2999 - Train Accuracy: 0.6078, Validation Accuracy: 0.6263, Loss: 1.4752\n",
      "Epoch  15 Batch 1600/2999 - Train Accuracy: 0.6409, Validation Accuracy: 0.6180, Loss: 1.3236\n",
      "Epoch  15 Batch 1700/2999 - Train Accuracy: 0.6420, Validation Accuracy: 0.6474, Loss: 1.2808\n",
      "Epoch  15 Batch 1800/2999 - Train Accuracy: 0.6110, Validation Accuracy: 0.6234, Loss: 1.4042\n",
      "Epoch  15 Batch 1900/2999 - Train Accuracy: 0.6431, Validation Accuracy: 0.6253, Loss: 1.3497\n",
      "Epoch  15 Batch 2000/2999 - Train Accuracy: 0.6286, Validation Accuracy: 0.6367, Loss: 1.2426\n",
      "Epoch  15 Batch 2100/2999 - Train Accuracy: 0.6492, Validation Accuracy: 0.6443, Loss: 1.2868\n",
      "Epoch  15 Batch 2200/2999 - Train Accuracy: 0.6199, Validation Accuracy: 0.6396, Loss: 1.4561\n",
      "Epoch  15 Batch 2300/2999 - Train Accuracy: 0.6448, Validation Accuracy: 0.6357, Loss: 1.2239\n",
      "Epoch  15 Batch 2400/2999 - Train Accuracy: 0.5996, Validation Accuracy: 0.6289, Loss: 1.4591\n",
      "Epoch  15 Batch 2500/2999 - Train Accuracy: 0.6292, Validation Accuracy: 0.6221, Loss: 1.3099\n",
      "Epoch  15 Batch 2600/2999 - Train Accuracy: 0.6390, Validation Accuracy: 0.6312, Loss: 1.2560\n",
      "Epoch  15 Batch 2700/2999 - Train Accuracy: 0.6234, Validation Accuracy: 0.6312, Loss: 1.3397\n",
      "Epoch  15 Batch 2800/2999 - Train Accuracy: 0.6312, Validation Accuracy: 0.6396, Loss: 1.2865\n",
      "Epoch  15 Batch 2900/2999 - Train Accuracy: 0.6336, Validation Accuracy: 0.6260, Loss: 1.3636\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  16 Batch  100/2999 - Train Accuracy: 0.6552, Validation Accuracy: 0.6299, Loss: 1.2348\n",
      "Epoch  16 Batch  200/2999 - Train Accuracy: 0.6199, Validation Accuracy: 0.6367, Loss: 1.4014\n",
      "Epoch  16 Batch  300/2999 - Train Accuracy: 0.6294, Validation Accuracy: 0.6302, Loss: 1.3645\n",
      "Epoch  16 Batch  400/2999 - Train Accuracy: 0.6076, Validation Accuracy: 0.6292, Loss: 1.5402\n",
      "Epoch  16 Batch  500/2999 - Train Accuracy: 0.6270, Validation Accuracy: 0.6339, Loss: 1.3662\n",
      "Epoch  16 Batch  600/2999 - Train Accuracy: 0.6696, Validation Accuracy: 0.6279, Loss: 1.1995\n",
      "Epoch  16 Batch  700/2999 - Train Accuracy: 0.6072, Validation Accuracy: 0.6253, Loss: 1.3766\n",
      "Epoch  16 Batch  800/2999 - Train Accuracy: 0.6228, Validation Accuracy: 0.6331, Loss: 1.3062\n",
      "Epoch  16 Batch  900/2999 - Train Accuracy: 0.6136, Validation Accuracy: 0.6195, Loss: 1.4080\n",
      "Epoch  16 Batch 1000/2999 - Train Accuracy: 0.6108, Validation Accuracy: 0.6430, Loss: 1.5514\n",
      "Epoch  16 Batch 1100/2999 - Train Accuracy: 0.6462, Validation Accuracy: 0.6471, Loss: 1.2695\n",
      "Epoch  16 Batch 1200/2999 - Train Accuracy: 0.6676, Validation Accuracy: 0.6339, Loss: 1.2478\n",
      "Epoch  16 Batch 1300/2999 - Train Accuracy: 0.6167, Validation Accuracy: 0.6229, Loss: 1.3645\n",
      "Epoch  16 Batch 1400/2999 - Train Accuracy: 0.6333, Validation Accuracy: 0.6227, Loss: 1.3038\n",
      "Epoch  16 Batch 1500/2999 - Train Accuracy: 0.6021, Validation Accuracy: 0.6471, Loss: 1.4443\n",
      "Epoch  16 Batch 1600/2999 - Train Accuracy: 0.6448, Validation Accuracy: 0.6258, Loss: 1.3307\n",
      "Epoch  16 Batch 1700/2999 - Train Accuracy: 0.6501, Validation Accuracy: 0.6242, Loss: 1.2759\n",
      "Epoch  16 Batch 1800/2999 - Train Accuracy: 0.6088, Validation Accuracy: 0.6359, Loss: 1.3667\n",
      "Epoch  16 Batch 1900/2999 - Train Accuracy: 0.6253, Validation Accuracy: 0.6315, Loss: 1.3418\n",
      "Epoch  16 Batch 2000/2999 - Train Accuracy: 0.6237, Validation Accuracy: 0.6242, Loss: 1.2197\n",
      "Epoch  16 Batch 2100/2999 - Train Accuracy: 0.6441, Validation Accuracy: 0.6365, Loss: 1.2889\n",
      "Epoch  16 Batch 2200/2999 - Train Accuracy: 0.6228, Validation Accuracy: 0.6411, Loss: 1.4349\n",
      "Epoch  16 Batch 2300/2999 - Train Accuracy: 0.6570, Validation Accuracy: 0.6422, Loss: 1.2207\n",
      "Epoch  16 Batch 2400/2999 - Train Accuracy: 0.5935, Validation Accuracy: 0.6284, Loss: 1.4426\n",
      "Epoch  16 Batch 2500/2999 - Train Accuracy: 0.6438, Validation Accuracy: 0.6323, Loss: 1.2999\n",
      "Epoch  16 Batch 2600/2999 - Train Accuracy: 0.6412, Validation Accuracy: 0.6378, Loss: 1.2716\n",
      "Epoch  16 Batch 2700/2999 - Train Accuracy: 0.6215, Validation Accuracy: 0.6229, Loss: 1.3466\n",
      "Epoch  16 Batch 2800/2999 - Train Accuracy: 0.6457, Validation Accuracy: 0.6292, Loss: 1.2773\n",
      "Epoch  16 Batch 2900/2999 - Train Accuracy: 0.6359, Validation Accuracy: 0.6229, Loss: 1.3933\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  17 Batch  100/2999 - Train Accuracy: 0.6664, Validation Accuracy: 0.6365, Loss: 1.2085\n",
      "Epoch  17 Batch  200/2999 - Train Accuracy: 0.6245, Validation Accuracy: 0.6333, Loss: 1.3793\n",
      "Epoch  17 Batch  300/2999 - Train Accuracy: 0.6182, Validation Accuracy: 0.6411, Loss: 1.3479\n",
      "Epoch  17 Batch  400/2999 - Train Accuracy: 0.6109, Validation Accuracy: 0.6435, Loss: 1.5254\n",
      "Epoch  17 Batch  500/2999 - Train Accuracy: 0.6267, Validation Accuracy: 0.6438, Loss: 1.3475\n",
      "Epoch  17 Batch  600/2999 - Train Accuracy: 0.6620, Validation Accuracy: 0.6328, Loss: 1.1890\n",
      "Epoch  17 Batch  700/2999 - Train Accuracy: 0.6048, Validation Accuracy: 0.6177, Loss: 1.3772\n",
      "Epoch  17 Batch  800/2999 - Train Accuracy: 0.6317, Validation Accuracy: 0.6333, Loss: 1.2943\n",
      "Epoch  17 Batch  900/2999 - Train Accuracy: 0.6063, Validation Accuracy: 0.6315, Loss: 1.3943\n",
      "Epoch  17 Batch 1000/2999 - Train Accuracy: 0.6046, Validation Accuracy: 0.6320, Loss: 1.5269\n",
      "Epoch  17 Batch 1100/2999 - Train Accuracy: 0.6479, Validation Accuracy: 0.6461, Loss: 1.2513\n",
      "Epoch  17 Batch 1200/2999 - Train Accuracy: 0.6638, Validation Accuracy: 0.6406, Loss: 1.2530\n",
      "Epoch  17 Batch 1300/2999 - Train Accuracy: 0.6430, Validation Accuracy: 0.6336, Loss: 1.3546\n",
      "Epoch  17 Batch 1400/2999 - Train Accuracy: 0.6378, Validation Accuracy: 0.6146, Loss: 1.2827\n",
      "Epoch  17 Batch 1500/2999 - Train Accuracy: 0.6083, Validation Accuracy: 0.6448, Loss: 1.4692\n",
      "Epoch  17 Batch 1600/2999 - Train Accuracy: 0.6443, Validation Accuracy: 0.6198, Loss: 1.3109\n",
      "Epoch  17 Batch 1700/2999 - Train Accuracy: 0.6544, Validation Accuracy: 0.6292, Loss: 1.2652\n",
      "Epoch  17 Batch 1800/2999 - Train Accuracy: 0.6359, Validation Accuracy: 0.6336, Loss: 1.3794\n",
      "Epoch  17 Batch 1900/2999 - Train Accuracy: 0.6341, Validation Accuracy: 0.6286, Loss: 1.3447\n",
      "Epoch  17 Batch 2000/2999 - Train Accuracy: 0.6120, Validation Accuracy: 0.6320, Loss: 1.2065\n",
      "Epoch  17 Batch 2100/2999 - Train Accuracy: 0.6455, Validation Accuracy: 0.6255, Loss: 1.2656\n",
      "Epoch  17 Batch 2200/2999 - Train Accuracy: 0.6196, Validation Accuracy: 0.6237, Loss: 1.4046\n",
      "Epoch  17 Batch 2300/2999 - Train Accuracy: 0.6568, Validation Accuracy: 0.6378, Loss: 1.1895\n",
      "Epoch  17 Batch 2400/2999 - Train Accuracy: 0.6069, Validation Accuracy: 0.6331, Loss: 1.4258\n",
      "Epoch  17 Batch 2500/2999 - Train Accuracy: 0.6279, Validation Accuracy: 0.6354, Loss: 1.2789\n",
      "Epoch  17 Batch 2600/2999 - Train Accuracy: 0.6342, Validation Accuracy: 0.6250, Loss: 1.2347\n",
      "Epoch  17 Batch 2700/2999 - Train Accuracy: 0.6239, Validation Accuracy: 0.6180, Loss: 1.3103\n",
      "Epoch  17 Batch 2800/2999 - Train Accuracy: 0.6498, Validation Accuracy: 0.6310, Loss: 1.2581\n",
      "Epoch  17 Batch 2900/2999 - Train Accuracy: 0.6353, Validation Accuracy: 0.6180, Loss: 1.3558\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  18 Batch  100/2999 - Train Accuracy: 0.6695, Validation Accuracy: 0.6297, Loss: 1.2042\n",
      "Epoch  18 Batch  200/2999 - Train Accuracy: 0.6261, Validation Accuracy: 0.6409, Loss: 1.3667\n",
      "Epoch  18 Batch  300/2999 - Train Accuracy: 0.6242, Validation Accuracy: 0.6299, Loss: 1.3471\n",
      "Epoch  18 Batch  400/2999 - Train Accuracy: 0.6129, Validation Accuracy: 0.6216, Loss: 1.5097\n",
      "Epoch  18 Batch  500/2999 - Train Accuracy: 0.6212, Validation Accuracy: 0.6354, Loss: 1.3411\n",
      "Epoch  18 Batch  600/2999 - Train Accuracy: 0.6603, Validation Accuracy: 0.6013, Loss: 1.1804\n",
      "Epoch  18 Batch  700/2999 - Train Accuracy: 0.6061, Validation Accuracy: 0.6148, Loss: 1.3636\n",
      "Epoch  18 Batch  800/2999 - Train Accuracy: 0.6286, Validation Accuracy: 0.6201, Loss: 1.2805\n",
      "Epoch  18 Batch  900/2999 - Train Accuracy: 0.6214, Validation Accuracy: 0.6211, Loss: 1.3780\n",
      "Epoch  18 Batch 1000/2999 - Train Accuracy: 0.6130, Validation Accuracy: 0.6271, Loss: 1.5141\n",
      "Epoch  18 Batch 1100/2999 - Train Accuracy: 0.6439, Validation Accuracy: 0.6310, Loss: 1.2570\n",
      "Epoch  18 Batch 1200/2999 - Train Accuracy: 0.6638, Validation Accuracy: 0.6250, Loss: 1.2333\n",
      "Epoch  18 Batch 1300/2999 - Train Accuracy: 0.6359, Validation Accuracy: 0.6234, Loss: 1.3447\n",
      "Epoch  18 Batch 1400/2999 - Train Accuracy: 0.6414, Validation Accuracy: 0.6276, Loss: 1.2774\n",
      "Epoch  18 Batch 1500/2999 - Train Accuracy: 0.6053, Validation Accuracy: 0.6357, Loss: 1.4345\n",
      "Epoch  18 Batch 1600/2999 - Train Accuracy: 0.6385, Validation Accuracy: 0.6151, Loss: 1.2994\n",
      "Epoch  18 Batch 1700/2999 - Train Accuracy: 0.6530, Validation Accuracy: 0.6294, Loss: 1.2507\n",
      "Epoch  18 Batch 1800/2999 - Train Accuracy: 0.6295, Validation Accuracy: 0.6331, Loss: 1.3675\n",
      "Epoch  18 Batch 1900/2999 - Train Accuracy: 0.6341, Validation Accuracy: 0.6294, Loss: 1.3165\n",
      "Epoch  18 Batch 2000/2999 - Train Accuracy: 0.6367, Validation Accuracy: 0.6237, Loss: 1.2088\n",
      "Epoch  18 Batch 2100/2999 - Train Accuracy: 0.6495, Validation Accuracy: 0.6359, Loss: 1.2662\n",
      "Epoch  18 Batch 2200/2999 - Train Accuracy: 0.6339, Validation Accuracy: 0.6385, Loss: 1.3988\n",
      "Epoch  18 Batch 2300/2999 - Train Accuracy: 0.6503, Validation Accuracy: 0.6305, Loss: 1.1829\n",
      "Epoch  18 Batch 2400/2999 - Train Accuracy: 0.6077, Validation Accuracy: 0.6221, Loss: 1.4114\n",
      "Epoch  18 Batch 2500/2999 - Train Accuracy: 0.6281, Validation Accuracy: 0.6289, Loss: 1.2698\n",
      "Epoch  18 Batch 2600/2999 - Train Accuracy: 0.6466, Validation Accuracy: 0.6255, Loss: 1.2493\n",
      "Epoch  18 Batch 2700/2999 - Train Accuracy: 0.6207, Validation Accuracy: 0.6164, Loss: 1.3059\n",
      "Epoch  18 Batch 2800/2999 - Train Accuracy: 0.6401, Validation Accuracy: 0.6346, Loss: 1.2461\n",
      "Epoch  18 Batch 2900/2999 - Train Accuracy: 0.6345, Validation Accuracy: 0.6333, Loss: 1.3332\n",
      "loss saved\n",
      "\n",
      "Model Saved\n",
      "Epoch  19 Batch  100/2999 - Train Accuracy: 0.6789, Validation Accuracy: 0.6326, Loss: 1.1797\n",
      "Epoch  19 Batch  200/2999 - Train Accuracy: 0.6250, Validation Accuracy: 0.6326, Loss: 1.3645\n",
      "Epoch  19 Batch  300/2999 - Train Accuracy: 0.6359, Validation Accuracy: 0.6279, Loss: 1.3345\n",
      "Epoch  19 Batch  400/2999 - Train Accuracy: 0.6190, Validation Accuracy: 0.6310, Loss: 1.5157\n",
      "Epoch  19 Batch  500/2999 - Train Accuracy: 0.6236, Validation Accuracy: 0.6258, Loss: 1.3399\n",
      "Epoch  19 Batch  600/2999 - Train Accuracy: 0.6525, Validation Accuracy: 0.6349, Loss: 1.1775\n",
      "Epoch  19 Batch  700/2999 - Train Accuracy: 0.6094, Validation Accuracy: 0.6148, Loss: 1.3512\n",
      "Epoch  19 Batch  800/2999 - Train Accuracy: 0.6289, Validation Accuracy: 0.6232, Loss: 1.2512\n",
      "Epoch  19 Batch  900/2999 - Train Accuracy: 0.6264, Validation Accuracy: 0.6323, Loss: 1.3642\n",
      "Epoch  19 Batch 1000/2999 - Train Accuracy: 0.6007, Validation Accuracy: 0.6333, Loss: 1.5175\n",
      "Epoch  19 Batch 1100/2999 - Train Accuracy: 0.6585, Validation Accuracy: 0.6458, Loss: 1.2339\n",
      "Epoch  19 Batch 1200/2999 - Train Accuracy: 0.6673, Validation Accuracy: 0.6346, Loss: 1.2176\n",
      "Epoch  19 Batch 1300/2999 - Train Accuracy: 0.6424, Validation Accuracy: 0.6362, Loss: 1.3401\n",
      "Epoch  19 Batch 1400/2999 - Train Accuracy: 0.6440, Validation Accuracy: 0.6333, Loss: 1.2679\n",
      "Epoch  19 Batch 1500/2999 - Train Accuracy: 0.6145, Validation Accuracy: 0.6336, Loss: 1.4227\n",
      "Epoch  19 Batch 1600/2999 - Train Accuracy: 0.6492, Validation Accuracy: 0.6060, Loss: 1.2779\n",
      "Epoch  19 Batch 1700/2999 - Train Accuracy: 0.6630, Validation Accuracy: 0.6385, Loss: 1.2123\n",
      "Epoch  19 Batch 1800/2999 - Train Accuracy: 0.6286, Validation Accuracy: 0.6474, Loss: 1.3481\n",
      "Epoch  19 Batch 1900/2999 - Train Accuracy: 0.6497, Validation Accuracy: 0.6411, Loss: 1.2873\n",
      "Epoch  19 Batch 2000/2999 - Train Accuracy: 0.6380, Validation Accuracy: 0.6266, Loss: 1.1893\n",
      "Epoch  19 Batch 2100/2999 - Train Accuracy: 0.6511, Validation Accuracy: 0.6372, Loss: 1.2418\n",
      "Epoch  19 Batch 2200/2999 - Train Accuracy: 0.6317, Validation Accuracy: 0.6424, Loss: 1.4057\n",
      "Epoch  19 Batch 2300/2999 - Train Accuracy: 0.6565, Validation Accuracy: 0.6326, Loss: 1.1852\n",
      "Epoch  19 Batch 2400/2999 - Train Accuracy: 0.6066, Validation Accuracy: 0.6365, Loss: 1.3912\n",
      "Epoch  19 Batch 2500/2999 - Train Accuracy: 0.6479, Validation Accuracy: 0.6326, Loss: 1.2796\n",
      "Epoch  19 Batch 2600/2999 - Train Accuracy: 0.6471, Validation Accuracy: 0.6396, Loss: 1.2225\n",
      "Epoch  19 Batch 2700/2999 - Train Accuracy: 0.6202, Validation Accuracy: 0.6253, Loss: 1.2914\n",
      "Epoch  19 Batch 2800/2999 - Train Accuracy: 0.6441, Validation Accuracy: 0.6266, Loss: 1.2347\n",
      "Epoch  19 Batch 2900/2999 - Train Accuracy: 0.6350, Validation Accuracy: 0.6396, Loss: 1.3225\n",
      "loss saved\n",
      "\n",
      "Model Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-459ca2738d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m                  \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                  \u001b[0mtarget_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                  keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "                \n",
    "        loss_file=open(\"/content/drive/My Drive/English DataSet/OfficialOneSideEnglishTensorflow.txt\", \"a+\")\n",
    "        loss_file.write(str(loss)+'\\n')\n",
    "        loss_file.close()\n",
    "        print(\"loss saved\\n\")\n",
    "\n",
    "        # Save Model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path)\n",
    "        print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAU2iItTyCr3"
   },
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('/content/drive/My Drive/TenWithoutAtt/params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjbl1lJyyFq3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save parameters for checkpoint\n",
    "save_params(save_path)\n",
    "\n",
    "!cp params.p /content/drive/My\\ Drive/TenWithoutAtt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWXUwHu2yHul"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unittest as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dmoCQYHIbsOl",
    "outputId": "e57980ba-e416-4140-bcb2-40756ca50773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/TenWithoutAtt/dev\n"
     ]
    }
   ],
   "source": [
    "print(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "hfNquYJ2yNTK",
    "outputId": "744f9f0e-8642-48d4-fa68-38b57691a6f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/TenWithoutAtt/dev\n",
      "Input\n",
      "  Word Ids:      [49759, 57242, 24343, 53286, 38059, 53286, 1860, 3772, 38927, 52952, 31836, 37682, 2]\n",
      "  input Words: ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', '<UNK>']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [4127, 49322, 16266, 15793, 16266, 15793, 16266, 15793, 16266, 15793, 16266, 15793, 1928, 1448, 39628, 34966, 8630, 42268, 49200, 7473, 16266, 16266, 1]\n",
      "  output Words: bout gruop tulare unsc tulare unsc tulare unsc tulare unsc tulare unsc yesterday internals vacate chloro acclimate slideshow rooftop shri tulare tulare <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "paraphrasing_sentence = 'what is the step by step guide to invest in share market '\n",
    "\n",
    "paraphrasing_sentence = sentence_to_seq(paraphrasing_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    paraphrasing_logits = sess.run(logits, {input_data: [paraphrasing_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(paraphrasing_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in paraphrasing_sentence]))\n",
    "print('  phrase Words: {}'.format([source_int_to_vocab[i] for i in paraphrasing_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in paraphrasing_logits]))\n",
    "print('  paraphrase Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in paraphrasing_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Tor-BYWkzA-V",
    "outputId": "b2aa1ca4-2445-4f8f-db07-3ac69add6fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "64999\n",
      "65000\n",
      "65001\n",
      "65002\n",
      "65003\n",
      "65004\n",
      "65005\n",
      "65006\n",
      "65007\n",
      "65008\n",
      "65009\n",
      "65010\n",
      "65011\n",
      "65012\n",
      "65013\n",
      "65014\n",
      "65015\n",
      "65016\n",
      "65017\n",
      "65018\n",
      "65019\n",
      "65020\n",
      "65021\n",
      "65022\n",
      "65023\n",
      "65024\n",
      "65025\n",
      "65026\n",
      "65027\n",
      "65028\n",
      "65029\n",
      "65030\n",
      "65031\n",
      "65032\n",
      "65033\n",
      "65034\n",
      "65035\n",
      "65036\n",
      "65037\n",
      "65038\n",
      "65039\n",
      "65040\n",
      "65041\n",
      "65042\n",
      "65043\n",
      "65044\n",
      "65045\n",
      "65046\n",
      "65047\n",
      "65048\n",
      "65049\n",
      "65050\n",
      "65051\n",
      "65052\n",
      "65053\n",
      "65054\n",
      "65055\n",
      "65056\n",
      "65057\n",
      "65058\n",
      "65059\n",
      "65060\n",
      "65061\n",
      "65062\n",
      "65063\n",
      "65064\n",
      "65065\n",
      "65066\n",
      "65067\n",
      "65068\n",
      "65069\n",
      "65070\n",
      "65071\n",
      "65072\n",
      "65073\n",
      "65074\n",
      "65075\n",
      "65076\n",
      "65077\n",
      "65078\n",
      "65079\n",
      "65080\n",
      "65081\n",
      "65082\n",
      "65083\n",
      "65084\n",
      "65085\n",
      "65086\n",
      "65087\n",
      "65088\n",
      "65089\n",
      "65090\n",
      "65091\n",
      "65092\n",
      "65093\n",
      "65094\n",
      "65095\n",
      "65096\n",
      "65097\n",
      "65098\n",
      "65099\n",
      "65100\n",
      "65101\n",
      "65102\n",
      "65103\n",
      "65104\n",
      "65105\n",
      "65106\n",
      "65107\n",
      "65108\n",
      "65109\n",
      "65110\n",
      "65111\n",
      "65112\n",
      "65113\n",
      "65114\n",
      "65115\n",
      "65116\n",
      "65117\n",
      "65118\n",
      "65119\n",
      "65120\n",
      "65121\n",
      "65122\n",
      "65123\n",
      "65124\n",
      "65125\n",
      "65126\n",
      "65127\n",
      "65128\n",
      "65129\n",
      "65130\n",
      "65131\n",
      "65132\n",
      "65133\n",
      "65134\n",
      "65135\n",
      "65136\n",
      "65137\n",
      "65138\n",
      "65139\n",
      "65140\n",
      "65141\n",
      "65142\n",
      "65143\n",
      "65144\n",
      "65145\n",
      "65146\n",
      "65147\n",
      "65148\n",
      "65149\n",
      "65150\n",
      "65151\n",
      "65152\n",
      "65153\n",
      "65154\n",
      "65155\n",
      "65156\n",
      "65157\n",
      "65158\n",
      "65159\n",
      "65160\n",
      "65161\n",
      "65162\n",
      "65163\n",
      "65164\n",
      "65165\n",
      "65166\n",
      "65167\n",
      "65168\n",
      "65169\n",
      "65170\n",
      "65171\n",
      "65172\n",
      "65173\n",
      "65174\n",
      "65175\n",
      "65176\n",
      "65177\n",
      "65178\n",
      "65179\n",
      "65180\n",
      "65181\n",
      "65182\n",
      "65183\n",
      "65184\n",
      "65185\n",
      "65186\n",
      "65187\n",
      "65188\n",
      "65189\n",
      "65190\n",
      "65191\n",
      "65192\n",
      "65193\n",
      "65194\n",
      "65195\n",
      "65196\n",
      "65197\n",
      "65198\n",
      "65199\n",
      "65200\n",
      "65201\n",
      "65202\n",
      "65203\n",
      "65204\n",
      "65205\n",
      "65206\n",
      "65207\n",
      "65208\n",
      "65209\n",
      "65210\n",
      "65211\n",
      "65212\n",
      "65213\n",
      "65214\n",
      "65215\n",
      "65216\n",
      "65217\n",
      "65218\n",
      "65219\n",
      "65220\n",
      "65221\n",
      "65222\n",
      "65223\n",
      "65224\n",
      "65225\n",
      "65226\n",
      "65227\n",
      "65228\n",
      "65229\n",
      "65230\n",
      "65231\n",
      "65232\n",
      "65233\n",
      "65234\n",
      "65235\n",
      "65236\n",
      "65237\n",
      "65238\n",
      "65239\n",
      "65240\n",
      "65241\n",
      "65242\n",
      "65243\n",
      "65244\n",
      "65245\n",
      "65246\n",
      "65247\n",
      "65248\n",
      "65249\n",
      "65250\n",
      "65251\n",
      "65252\n",
      "65253\n",
      "65254\n",
      "65255\n",
      "65256\n",
      "65257\n",
      "65258\n",
      "65259\n",
      "65260\n",
      "65261\n",
      "65262\n",
      "65263\n",
      "65264\n",
      "65265\n",
      "65266\n",
      "65267\n",
      "65268\n",
      "65269\n",
      "65270\n",
      "65271\n",
      "65272\n",
      "65273\n",
      "65274\n",
      "65275\n",
      "65276\n",
      "65277\n",
      "65278\n",
      "65279\n",
      "65280\n",
      "65281\n",
      "65282\n",
      "65283\n",
      "65284\n",
      "65285\n",
      "65286\n",
      "65287\n",
      "65288\n",
      "65289\n",
      "65290\n",
      "65291\n",
      "65292\n",
      "65293\n",
      "65294\n",
      "65295\n",
      "65296\n",
      "65297\n",
      "65298\n",
      "65299\n",
      "65300\n",
      "65301\n",
      "65302\n",
      "65303\n",
      "65304\n",
      "65305\n",
      "65306\n",
      "65307\n",
      "65308\n",
      "65309\n",
      "65310\n",
      "65311\n",
      "65312\n",
      "65313\n",
      "65314\n",
      "65315\n",
      "65316\n",
      "65317\n",
      "65318\n",
      "65319\n",
      "65320\n",
      "65321\n",
      "65322\n",
      "65323\n",
      "65324\n",
      "65325\n",
      "65326\n",
      "65327\n",
      "65328\n",
      "65329\n",
      "65330\n",
      "65331\n",
      "65332\n",
      "65333\n",
      "65334\n",
      "65335\n",
      "65336\n",
      "65337\n",
      "65338\n",
      "65339\n",
      "65340\n",
      "65341\n",
      "65342\n",
      "65343\n",
      "65344\n",
      "65345\n",
      "65346\n",
      "65347\n",
      "65348\n",
      "65349\n",
      "65350\n",
      "65351\n",
      "65352\n",
      "65353\n",
      "65354\n",
      "65355\n",
      "65356\n",
      "65357\n",
      "65358\n",
      "65359\n",
      "65360\n",
      "65361\n",
      "65362\n",
      "65363\n",
      "65364\n",
      "65365\n",
      "65366\n",
      "65367\n",
      "65368\n",
      "65369\n",
      "65370\n",
      "65371\n",
      "65372\n",
      "65373\n",
      "65374\n",
      "65375\n",
      "65376\n",
      "65377\n",
      "65378\n",
      "65379\n",
      "65380\n",
      "65381\n",
      "65382\n",
      "65383\n",
      "65384\n",
      "65385\n",
      "65386\n",
      "65387\n",
      "65388\n",
      "65389\n",
      "65390\n",
      "65391\n",
      "65392\n",
      "65393\n",
      "65394\n",
      "65395\n",
      "65396\n",
      "65397\n",
      "65398\n",
      "65399\n",
      "65400\n",
      "65401\n",
      "65402\n",
      "65403\n",
      "65404\n",
      "65405\n",
      "65406\n",
      "65407\n",
      "65408\n",
      "65409\n",
      "65410\n",
      "65411\n",
      "65412\n",
      "65413\n",
      "65414\n",
      "65415\n",
      "65416\n",
      "65417\n",
      "65418\n",
      "65419\n",
      "65420\n",
      "65421\n",
      "65422\n",
      "65423\n",
      "65424\n",
      "65425\n",
      "65426\n",
      "65427\n",
      "65428\n",
      "65429\n",
      "65430\n",
      "65431\n",
      "65432\n",
      "65433\n",
      "65434\n",
      "65435\n",
      "65436\n",
      "65437\n",
      "65438\n",
      "65439\n",
      "65440\n",
      "65441\n",
      "65442\n",
      "65443\n",
      "65444\n",
      "65445\n",
      "65446\n",
      "65447\n",
      "65448\n",
      "65449\n",
      "65450\n",
      "65451\n",
      "65452\n",
      "65453\n",
      "65454\n",
      "65455\n",
      "65456\n",
      "65457\n",
      "65458\n",
      "65459\n",
      "65460\n",
      "65461\n",
      "65462\n",
      "65463\n",
      "65464\n",
      "65465\n",
      "65466\n",
      "65467\n",
      "65468\n",
      "65469\n",
      "65470\n",
      "65471\n",
      "65472\n",
      "65473\n",
      "65474\n",
      "65475\n",
      "65476\n",
      "65477\n",
      "65478\n",
      "65479\n",
      "65480\n",
      "65481\n",
      "65482\n",
      "65483\n",
      "65484\n",
      "65485\n",
      "65486\n",
      "65487\n",
      "65488\n",
      "65489\n",
      "65490\n",
      "65491\n",
      "65492\n",
      "65493\n",
      "65494\n",
      "65495\n",
      "65496\n",
      "65497\n",
      "65498\n",
      "65499\n",
      "65500\n",
      "65501\n",
      "65502\n",
      "65503\n",
      "65504\n",
      "65505\n",
      "65506\n",
      "65507\n",
      "65508\n",
      "65509\n",
      "65510\n",
      "65511\n",
      "65512\n",
      "65513\n",
      "65514\n",
      "65515\n",
      "65516\n",
      "65517\n",
      "65518\n",
      "65519\n",
      "65520\n",
      "65521\n",
      "65522\n",
      "65523\n",
      "65524\n",
      "65525\n",
      "65526\n",
      "65527\n",
      "65528\n",
      "65529\n",
      "65530\n",
      "65531\n",
      "65532\n",
      "65533\n",
      "65534\n",
      "65535\n",
      "65536\n",
      "65537\n",
      "65538\n",
      "65539\n",
      "65540\n",
      "65541\n",
      "65542\n",
      "65543\n",
      "65544\n",
      "65545\n",
      "65546\n",
      "65547\n",
      "65548\n",
      "65549\n",
      "65550\n",
      "65551\n",
      "65552\n",
      "65553\n",
      "65554\n",
      "65555\n",
      "65556\n",
      "65557\n",
      "65558\n",
      "65559\n",
      "65560\n",
      "65561\n",
      "65562\n",
      "65563\n",
      "65564\n",
      "65565\n",
      "65566\n",
      "65567\n",
      "65568\n",
      "65569\n",
      "65570\n",
      "65571\n",
      "65572\n",
      "65573\n",
      "65574\n",
      "65575\n",
      "65576\n",
      "65577\n",
      "65578\n",
      "65579\n",
      "65580\n",
      "65581\n",
      "65582\n",
      "65583\n",
      "65584\n",
      "65585\n",
      "65586\n",
      "65587\n",
      "65588\n",
      "65589\n",
      "65590\n",
      "65591\n",
      "65592\n",
      "65593\n",
      "65594\n",
      "65595\n",
      "65596\n",
      "65597\n",
      "65598\n",
      "65599\n",
      "65600\n",
      "65601\n",
      "65602\n",
      "65603\n",
      "65604\n",
      "65605\n",
      "65606\n",
      "65607\n",
      "65608\n",
      "65609\n",
      "65610\n",
      "65611\n",
      "65612\n",
      "65613\n",
      "65614\n",
      "65615\n",
      "65616\n",
      "65617\n",
      "65618\n",
      "65619\n",
      "65620\n",
      "65621\n",
      "65622\n",
      "65623\n",
      "65624\n",
      "65625\n",
      "65626\n",
      "65627\n",
      "65628\n",
      "65629\n",
      "65630\n",
      "65631\n",
      "65632\n",
      "65633\n",
      "65634\n",
      "65635\n",
      "65636\n",
      "65637\n",
      "65638\n",
      "65639\n",
      "65640\n",
      "65641\n",
      "65642\n",
      "65643\n",
      "65644\n",
      "65645\n",
      "65646\n",
      "65647\n",
      "65648\n",
      "65649\n",
      "65650\n",
      "65651\n",
      "65652\n",
      "65653\n",
      "65654\n",
      "65655\n",
      "65656\n",
      "65657\n",
      "65658\n",
      "65659\n",
      "65660\n",
      "65661\n",
      "65662\n",
      "65663\n",
      "65664\n",
      "65665\n",
      "65666\n",
      "65667\n",
      "65668\n",
      "65669\n",
      "65670\n",
      "65671\n",
      "65672\n",
      "65673\n",
      "65674\n",
      "65675\n",
      "65676\n",
      "65677\n",
      "65678\n",
      "65679\n",
      "65680\n",
      "65681\n",
      "65682\n",
      "65683\n",
      "65684\n",
      "65685\n",
      "65686\n",
      "65687\n",
      "65688\n",
      "65689\n",
      "65690\n",
      "65691\n",
      "65692\n",
      "65693\n",
      "65694\n",
      "65695\n",
      "65696\n",
      "65697\n",
      "65698\n",
      "65699\n",
      "65700\n",
      "65701\n",
      "65702\n",
      "65703\n",
      "65704\n",
      "65705\n",
      "65706\n",
      "65707\n",
      "65708\n",
      "65709\n",
      "65710\n",
      "65711\n",
      "65712\n",
      "65713\n",
      "65714\n",
      "65715\n",
      "65716\n",
      "65717\n",
      "65718\n",
      "65719\n",
      "65720\n",
      "65721\n",
      "65722\n",
      "65723\n",
      "65724\n",
      "65725\n",
      "65726\n",
      "65727\n",
      "65728\n",
      "65729\n",
      "65730\n",
      "65731\n",
      "65732\n",
      "65733\n",
      "65734\n",
      "65735\n",
      "65736\n",
      "65737\n",
      "65738\n",
      "65739\n",
      "65740\n",
      "65741\n",
      "65742\n",
      "65743\n",
      "65744\n",
      "65745\n",
      "65746\n",
      "65747\n",
      "65748\n",
      "65749\n",
      "65750\n",
      "65751\n",
      "65752\n",
      "65753\n",
      "65754\n",
      "65755\n",
      "65756\n",
      "65757\n",
      "65758\n",
      "65759\n",
      "65760\n",
      "65761\n",
      "65762\n",
      "65763\n",
      "65764\n",
      "65765\n",
      "65766\n",
      "65767\n",
      "65768\n",
      "65769\n",
      "65770\n",
      "65771\n",
      "65772\n",
      "65773\n",
      "65774\n",
      "65775\n",
      "65776\n",
      "65777\n",
      "65778\n",
      "65779\n",
      "65780\n",
      "65781\n",
      "65782\n",
      "65783\n",
      "65784\n",
      "65785\n",
      "65786\n",
      "65787\n",
      "65788\n",
      "65789\n",
      "65790\n",
      "65791\n",
      "65792\n",
      "65793\n",
      "65794\n",
      "65795\n",
      "65796\n",
      "65797\n",
      "65798\n",
      "65799\n",
      "65800\n",
      "65801\n",
      "65802\n",
      "65803\n",
      "65804\n",
      "65805\n",
      "65806\n",
      "65807\n",
      "65808\n",
      "65809\n",
      "65810\n",
      "65811\n",
      "65812\n",
      "65813\n",
      "65814\n",
      "65815\n",
      "65816\n",
      "65817\n",
      "65818\n",
      "65819\n",
      "65820\n",
      "65821\n",
      "65822\n",
      "65823\n",
      "65824\n",
      "65825\n",
      "65826\n",
      "65827\n",
      "65828\n",
      "65829\n",
      "65830\n",
      "65831\n",
      "65832\n",
      "65833\n",
      "65834\n",
      "65835\n",
      "65836\n",
      "65837\n",
      "65838\n",
      "65839\n",
      "65840\n",
      "65841\n",
      "65842\n",
      "65843\n",
      "65844\n",
      "65845\n",
      "65846\n",
      "65847\n",
      "65848\n",
      "65849\n",
      "65850\n",
      "65851\n",
      "65852\n",
      "65853\n",
      "65854\n",
      "65855\n",
      "65856\n",
      "65857\n",
      "65858\n",
      "65859\n",
      "65860\n",
      "65861\n",
      "65862\n",
      "65863\n",
      "65864\n",
      "65865\n",
      "65866\n",
      "65867\n",
      "65868\n",
      "65869\n",
      "65870\n",
      "65871\n",
      "65872\n",
      "65873\n",
      "65874\n",
      "65875\n",
      "65876\n",
      "65877\n",
      "65878\n",
      "65879\n",
      "65880\n",
      "65881\n",
      "65882\n",
      "65883\n",
      "65884\n",
      "65885\n",
      "65886\n",
      "65887\n",
      "65888\n",
      "65889\n",
      "65890\n",
      "65891\n",
      "65892\n",
      "65893\n",
      "65894\n",
      "65895\n",
      "65896\n",
      "65897\n",
      "65898\n",
      "65899\n",
      "65900\n",
      "65901\n",
      "65902\n",
      "65903\n",
      "65904\n",
      "65905\n",
      "65906\n",
      "65907\n",
      "65908\n",
      "65909\n",
      "65910\n",
      "65911\n",
      "65912\n",
      "65913\n",
      "65914\n",
      "65915\n",
      "65916\n",
      "65917\n",
      "65918\n",
      "65919\n",
      "65920\n",
      "65921\n",
      "65922\n",
      "65923\n",
      "65924\n",
      "65925\n",
      "65926\n",
      "65927\n",
      "65928\n",
      "65929\n",
      "65930\n",
      "65931\n",
      "65932\n",
      "65933\n",
      "65934\n",
      "65935\n",
      "65936\n",
      "65937\n",
      "65938\n",
      "65939\n",
      "65940\n",
      "65941\n",
      "65942\n",
      "65943\n",
      "65944\n",
      "65945\n",
      "65946\n",
      "65947\n",
      "65948\n",
      "65949\n",
      "65950\n",
      "65951\n",
      "65952\n",
      "65953\n",
      "65954\n",
      "65955\n",
      "65956\n",
      "65957\n",
      "65958\n",
      "65959\n",
      "65960\n",
      "65961\n",
      "65962\n",
      "65963\n",
      "65964\n",
      "65965\n",
      "65966\n",
      "65967\n",
      "65968\n",
      "65969\n",
      "65970\n",
      "65971\n",
      "65972\n",
      "65973\n",
      "65974\n",
      "65975\n",
      "65976\n",
      "65977\n",
      "65978\n",
      "65979\n",
      "65980\n",
      "65981\n",
      "65982\n",
      "65983\n",
      "65984\n",
      "65985\n",
      "65986\n",
      "65987\n",
      "65988\n",
      "65989\n",
      "65990\n",
      "65991\n",
      "65992\n",
      "65993\n",
      "65994\n",
      "65995\n",
      "65996\n",
      "65997\n",
      "65998\n",
      "65999\n",
      "66000\n",
      "66001\n",
      "66002\n",
      "66003\n",
      "66004\n",
      "66005\n",
      "66006\n",
      "66007\n",
      "66008\n",
      "66009\n",
      "66010\n",
      "66011\n",
      "66012\n",
      "66013\n",
      "66014\n",
      "66015\n",
      "66016\n",
      "66017\n",
      "66018\n",
      "66019\n",
      "66020\n",
      "66021\n",
      "66022\n",
      "66023\n",
      "66024\n",
      "66025\n",
      "66026\n",
      "66027\n",
      "66028\n",
      "66029\n",
      "66030\n",
      "66031\n",
      "66032\n",
      "66033\n",
      "66034\n",
      "66035\n",
      "66036\n",
      "66037\n",
      "66038\n",
      "66039\n",
      "66040\n",
      "66041\n",
      "66042\n",
      "66043\n",
      "66044\n",
      "66045\n",
      "66046\n",
      "66047\n",
      "66048\n",
      "66049\n",
      "66050\n",
      "66051\n",
      "66052\n",
      "66053\n",
      "66054\n",
      "66055\n",
      "66056\n",
      "66057\n",
      "66058\n",
      "66059\n",
      "66060\n",
      "66061\n",
      "66062\n",
      "66063\n",
      "66064\n",
      "66065\n",
      "66066\n",
      "66067\n",
      "66068\n",
      "66069\n",
      "66070\n",
      "66071\n",
      "66072\n",
      "66073\n",
      "66074\n",
      "66075\n",
      "66076\n",
      "66077\n",
      "66078\n",
      "66079\n",
      "66080\n",
      "66081\n",
      "66082\n",
      "66083\n",
      "66084\n",
      "66085\n",
      "66086\n",
      "66087\n",
      "66088\n",
      "66089\n",
      "66090\n",
      "66091\n",
      "66092\n",
      "66093\n",
      "66094\n",
      "66095\n",
      "66096\n",
      "66097\n",
      "66098\n",
      "66099\n",
      "66100\n",
      "66101\n",
      "66102\n",
      "66103\n",
      "66104\n",
      "66105\n",
      "66106\n",
      "66107\n",
      "66108\n",
      "66109\n",
      "66110\n",
      "66111\n",
      "66112\n",
      "66113\n",
      "66114\n",
      "66115\n",
      "66116\n",
      "66117\n",
      "66118\n",
      "66119\n",
      "66120\n",
      "66121\n",
      "66122\n",
      "66123\n",
      "66124\n",
      "66125\n",
      "66126\n",
      "66127\n",
      "66128\n",
      "66129\n",
      "66130\n",
      "66131\n",
      "66132\n",
      "66133\n",
      "66134\n",
      "66135\n",
      "66136\n",
      "66137\n",
      "66138\n",
      "66139\n",
      "66140\n",
      "66141\n",
      "66142\n",
      "66143\n",
      "66144\n",
      "66145\n",
      "66146\n",
      "66147\n",
      "66148\n",
      "66149\n",
      "66150\n",
      "66151\n",
      "66152\n",
      "66153\n",
      "66154\n",
      "66155\n",
      "66156\n",
      "66157\n",
      "66158\n",
      "66159\n",
      "66160\n",
      "66161\n",
      "66162\n",
      "66163\n",
      "66164\n",
      "66165\n",
      "66166\n",
      "66167\n",
      "66168\n",
      "66169\n",
      "66170\n",
      "66171\n",
      "66172\n",
      "66173\n",
      "66174\n",
      "66175\n",
      "66176\n",
      "66177\n",
      "66178\n",
      "66179\n",
      "66180\n",
      "66181\n",
      "66182\n",
      "66183\n",
      "66184\n",
      "66185\n",
      "66186\n",
      "66187\n",
      "66188\n",
      "66189\n",
      "66190\n",
      "66191\n",
      "66192\n",
      "66193\n",
      "66194\n",
      "66195\n",
      "66196\n",
      "66197\n",
      "66198\n",
      "66199\n",
      "66200\n",
      "66201\n",
      "66202\n",
      "66203\n",
      "66204\n",
      "66205\n",
      "66206\n",
      "66207\n",
      "66208\n",
      "66209\n",
      "66210\n",
      "66211\n",
      "66212\n",
      "66213\n",
      "66214\n",
      "66215\n",
      "66216\n",
      "66217\n",
      "66218\n",
      "66219\n",
      "66220\n",
      "66221\n",
      "66222\n",
      "66223\n",
      "66224\n",
      "66225\n",
      "66226\n",
      "66227\n",
      "66228\n",
      "66229\n",
      "66230\n",
      "66231\n",
      "66232\n",
      "66233\n",
      "66234\n",
      "66235\n",
      "66236\n",
      "66237\n",
      "66238\n",
      "66239\n",
      "66240\n",
      "66241\n",
      "66242\n",
      "66243\n",
      "66244\n",
      "66245\n",
      "66246\n",
      "66247\n",
      "66248\n",
      "66249\n",
      "66250\n",
      "66251\n",
      "66252\n",
      "66253\n",
      "66254\n",
      "66255\n",
      "66256\n",
      "66257\n",
      "66258\n",
      "66259\n",
      "66260\n",
      "66261\n",
      "66262\n",
      "66263\n",
      "66264\n",
      "66265\n",
      "66266\n",
      "66267\n",
      "66268\n",
      "66269\n",
      "66270\n",
      "66271\n",
      "66272\n",
      "66273\n",
      "66274\n",
      "66275\n",
      "66276\n",
      "66277\n",
      "66278\n",
      "66279\n",
      "66280\n",
      "66281\n",
      "66282\n",
      "66283\n",
      "66284\n",
      "66285\n",
      "66286\n",
      "66287\n",
      "66288\n",
      "66289\n",
      "66290\n",
      "66291\n",
      "66292\n",
      "66293\n",
      "66294\n",
      "66295\n",
      "66296\n",
      "66297\n",
      "66298\n",
      "66299\n",
      "66300\n",
      "66301\n",
      "66302\n",
      "66303\n",
      "66304\n",
      "66305\n",
      "66306\n",
      "66307\n",
      "66308\n",
      "66309\n",
      "66310\n",
      "66311\n",
      "66312\n",
      "66313\n",
      "66314\n",
      "66315\n",
      "66316\n",
      "66317\n",
      "66318\n",
      "66319\n",
      "66320\n",
      "66321\n",
      "66322\n",
      "66323\n",
      "66324\n",
      "66325\n",
      "66326\n",
      "66327\n",
      "66328\n",
      "66329\n",
      "66330\n",
      "66331\n",
      "66332\n",
      "66333\n",
      "66334\n",
      "66335\n",
      "66336\n",
      "66337\n",
      "66338\n",
      "66339\n",
      "66340\n",
      "66341\n",
      "66342\n",
      "66343\n",
      "66344\n",
      "66345\n",
      "66346\n",
      "66347\n",
      "66348\n",
      "66349\n",
      "66350\n",
      "66351\n",
      "66352\n",
      "66353\n",
      "66354\n",
      "66355\n",
      "66356\n",
      "66357\n",
      "66358\n",
      "66359\n",
      "66360\n",
      "66361\n",
      "66362\n",
      "66363\n",
      "66364\n",
      "66365\n",
      "66366\n",
      "66367\n",
      "66368\n",
      "66369\n",
      "66370\n",
      "66371\n",
      "66372\n",
      "66373\n",
      "66374\n",
      "66375\n",
      "66376\n",
      "66377\n",
      "66378\n",
      "66379\n",
      "66380\n",
      "66381\n",
      "66382\n",
      "66383\n",
      "66384\n",
      "66385\n",
      "66386\n",
      "66387\n",
      "66388\n",
      "66389\n",
      "66390\n",
      "66391\n",
      "66392\n",
      "66393\n",
      "66394\n",
      "66395\n",
      "66396\n",
      "66397\n",
      "66398\n",
      "66399\n",
      "66400\n",
      "66401\n",
      "66402\n",
      "66403\n",
      "66404\n",
      "66405\n",
      "66406\n",
      "66407\n",
      "66408\n",
      "66409\n",
      "66410\n",
      "66411\n",
      "66412\n",
      "66413\n",
      "66414\n",
      "66415\n",
      "66416\n",
      "66417\n",
      "66418\n",
      "66419\n",
      "66420\n",
      "66421\n",
      "66422\n",
      "66423\n",
      "66424\n",
      "66425\n",
      "66426\n",
      "66427\n",
      "66428\n",
      "66429\n",
      "66430\n",
      "66431\n",
      "66432\n",
      "66433\n",
      "66434\n",
      "66435\n",
      "66436\n",
      "66437\n",
      "66438\n",
      "66439\n",
      "66440\n",
      "66441\n",
      "66442\n",
      "66443\n",
      "66444\n",
      "66445\n",
      "66446\n",
      "66447\n",
      "66448\n",
      "66449\n",
      "66450\n",
      "66451\n",
      "66452\n",
      "66453\n",
      "66454\n",
      "66455\n",
      "66456\n",
      "66457\n",
      "66458\n",
      "66459\n",
      "66460\n",
      "66461\n",
      "66462\n",
      "66463\n",
      "66464\n",
      "66465\n",
      "66466\n",
      "66467\n",
      "66468\n",
      "66469\n",
      "66470\n",
      "66471\n",
      "66472\n",
      "66473\n",
      "66474\n",
      "66475\n",
      "66476\n",
      "66477\n",
      "66478\n",
      "66479\n",
      "66480\n",
      "66481\n",
      "66482\n",
      "66483\n",
      "66484\n",
      "66485\n",
      "66486\n",
      "66487\n",
      "66488\n",
      "66489\n",
      "66490\n",
      "66491\n",
      "66492\n",
      "66493\n",
      "66494\n",
      "66495\n",
      "66496\n",
      "66497\n",
      "66498\n",
      "66499\n",
      "66500\n",
      "66501\n",
      "66502\n",
      "66503\n",
      "66504\n",
      "66505\n",
      "66506\n",
      "66507\n",
      "66508\n",
      "66509\n",
      "66510\n",
      "66511\n",
      "66512\n",
      "66513\n",
      "66514\n",
      "66515\n",
      "66516\n",
      "66517\n",
      "66518\n",
      "66519\n",
      "66520\n",
      "66521\n",
      "66522\n",
      "66523\n",
      "66524\n",
      "66525\n",
      "66526\n",
      "66527\n",
      "66528\n",
      "66529\n",
      "66530\n",
      "66531\n",
      "66532\n",
      "66533\n",
      "66534\n",
      "66535\n",
      "66536\n",
      "66537\n",
      "66538\n",
      "66539\n",
      "66540\n",
      "66541\n",
      "66542\n",
      "66543\n",
      "66544\n",
      "66545\n",
      "66546\n",
      "66547\n",
      "66548\n",
      "66549\n",
      "66550\n",
      "66551\n",
      "66552\n",
      "66553\n",
      "66554\n",
      "66555\n",
      "66556\n",
      "66557\n",
      "66558\n",
      "66559\n",
      "66560\n",
      "66561\n",
      "66562\n",
      "66563\n",
      "66564\n",
      "66565\n",
      "66566\n",
      "66567\n",
      "66568\n",
      "66569\n",
      "66570\n",
      "66571\n",
      "66572\n",
      "66573\n",
      "66574\n",
      "66575\n",
      "66576\n",
      "66577\n",
      "66578\n",
      "66579\n",
      "66580\n",
      "66581\n",
      "66582\n",
      "66583\n",
      "66584\n",
      "66585\n",
      "66586\n",
      "66587\n",
      "66588\n",
      "66589\n",
      "66590\n",
      "66591\n",
      "66592\n",
      "66593\n",
      "66594\n",
      "66595\n",
      "66596\n",
      "66597\n",
      "66598\n",
      "66599\n",
      "66600\n",
      "66601\n",
      "66602\n",
      "66603\n",
      "66604\n",
      "66605\n",
      "66606\n",
      "66607\n",
      "66608\n",
      "66609\n",
      "66610\n",
      "66611\n",
      "66612\n",
      "66613\n",
      "66614\n",
      "66615\n",
      "66616\n",
      "66617\n",
      "66618\n",
      "66619\n",
      "66620\n",
      "66621\n",
      "66622\n",
      "66623\n",
      "66624\n",
      "66625\n",
      "66626\n",
      "66627\n",
      "66628\n",
      "66629\n",
      "66630\n",
      "66631\n",
      "66632\n",
      "66633\n",
      "66634\n",
      "66635\n",
      "66636\n",
      "66637\n",
      "66638\n",
      "66639\n",
      "66640\n",
      "66641\n",
      "66642\n",
      "66643\n",
      "66644\n",
      "66645\n",
      "66646\n",
      "66647\n",
      "66648\n",
      "66649\n",
      "66650\n",
      "66651\n",
      "66652\n",
      "66653\n",
      "66654\n",
      "66655\n",
      "66656\n",
      "66657\n",
      "66658\n",
      "66659\n",
      "66660\n",
      "66661\n",
      "66662\n",
      "66663\n",
      "66664\n",
      "66665\n",
      "66666\n",
      "66667\n",
      "66668\n",
      "66669\n",
      "66670\n",
      "66671\n",
      "66672\n",
      "66673\n",
      "66674\n",
      "66675\n",
      "66676\n",
      "66677\n",
      "66678\n",
      "66679\n",
      "66680\n",
      "66681\n",
      "66682\n",
      "66683\n",
      "66684\n",
      "66685\n",
      "66686\n",
      "66687\n",
      "66688\n",
      "66689\n",
      "66690\n",
      "66691\n",
      "66692\n",
      "66693\n",
      "66694\n",
      "66695\n",
      "66696\n",
      "66697\n",
      "66698\n",
      "66699\n",
      "66700\n",
      "66701\n",
      "66702\n",
      "66703\n",
      "66704\n",
      "66705\n",
      "66706\n",
      "66707\n",
      "66708\n",
      "66709\n",
      "66710\n",
      "66711\n",
      "66712\n",
      "66713\n",
      "66714\n",
      "66715\n",
      "66716\n",
      "66717\n",
      "66718\n",
      "66719\n",
      "66720\n",
      "66721\n",
      "66722\n",
      "66723\n",
      "66724\n",
      "66725\n",
      "66726\n",
      "66727\n",
      "66728\n",
      "66729\n",
      "66730\n",
      "66731\n",
      "66732\n",
      "66733\n",
      "66734\n",
      "66735\n",
      "66736\n",
      "66737\n",
      "66738\n",
      "66739\n",
      "66740\n",
      "66741\n",
      "66742\n",
      "66743\n",
      "66744\n",
      "66745\n",
      "66746\n",
      "66747\n",
      "66748\n",
      "66749\n",
      "66750\n",
      "66751\n",
      "66752\n",
      "66753\n",
      "66754\n",
      "66755\n",
      "66756\n",
      "66757\n",
      "66758\n",
      "66759\n",
      "66760\n",
      "66761\n",
      "66762\n",
      "66763\n",
      "66764\n",
      "66765\n",
      "66766\n",
      "66767\n",
      "66768\n",
      "66769\n",
      "66770\n",
      "66771\n",
      "66772\n",
      "66773\n",
      "66774\n",
      "66775\n",
      "66776\n",
      "66777\n",
      "66778\n",
      "66779\n",
      "66780\n",
      "66781\n",
      "66782\n",
      "66783\n",
      "66784\n",
      "66785\n",
      "66786\n",
      "66787\n",
      "66788\n",
      "66789\n",
      "66790\n",
      "66791\n",
      "66792\n",
      "66793\n",
      "66794\n",
      "66795\n",
      "66796\n",
      "66797\n",
      "66798\n",
      "66799\n",
      "66800\n",
      "66801\n",
      "66802\n",
      "66803\n",
      "66804\n",
      "66805\n",
      "66806\n",
      "66807\n",
      "66808\n",
      "66809\n",
      "66810\n",
      "66811\n",
      "66812\n",
      "66813\n",
      "66814\n",
      "66815\n",
      "66816\n",
      "66817\n",
      "66818\n",
      "66819\n",
      "66820\n",
      "66821\n",
      "66822\n",
      "66823\n",
      "66824\n",
      "66825\n",
      "66826\n",
      "66827\n",
      "66828\n",
      "66829\n",
      "66830\n",
      "66831\n",
      "66832\n",
      "66833\n",
      "66834\n",
      "66835\n",
      "66836\n",
      "66837\n",
      "66838\n",
      "66839\n",
      "66840\n",
      "66841\n",
      "66842\n",
      "66843\n",
      "66844\n",
      "66845\n",
      "66846\n",
      "66847\n",
      "66848\n",
      "66849\n",
      "66850\n",
      "66851\n",
      "66852\n",
      "66853\n",
      "66854\n",
      "66855\n",
      "66856\n",
      "66857\n",
      "66858\n",
      "66859\n",
      "66860\n",
      "66861\n",
      "66862\n",
      "66863\n",
      "66864\n",
      "66865\n",
      "66866\n",
      "66867\n",
      "66868\n",
      "66869\n",
      "66870\n",
      "66871\n",
      "66872\n",
      "66873\n",
      "66874\n",
      "66875\n",
      "66876\n",
      "66877\n",
      "66878\n",
      "66879\n",
      "66880\n",
      "66881\n",
      "66882\n",
      "66883\n",
      "66884\n",
      "66885\n",
      "66886\n",
      "66887\n",
      "66888\n",
      "66889\n",
      "66890\n",
      "66891\n",
      "66892\n",
      "66893\n",
      "66894\n",
      "66895\n",
      "66896\n",
      "66897\n",
      "66898\n",
      "66899\n",
      "66900\n",
      "66901\n",
      "66902\n",
      "66903\n",
      "66904\n",
      "66905\n",
      "66906\n",
      "66907\n",
      "66908\n",
      "66909\n",
      "66910\n",
      "66911\n",
      "66912\n",
      "66913\n",
      "66914\n",
      "66915\n",
      "66916\n",
      "66917\n",
      "66918\n",
      "66919\n",
      "66920\n",
      "66921\n",
      "66922\n",
      "66923\n",
      "66924\n",
      "66925\n",
      "66926\n",
      "66927\n",
      "66928\n",
      "66929\n",
      "66930\n",
      "66931\n",
      "66932\n",
      "66933\n",
      "66934\n",
      "66935\n",
      "66936\n",
      "66937\n",
      "66938\n",
      "66939\n",
      "66940\n",
      "66941\n",
      "66942\n",
      "66943\n",
      "66944\n",
      "66945\n",
      "66946\n",
      "66947\n",
      "66948\n",
      "66949\n",
      "66950\n",
      "66951\n",
      "66952\n",
      "66953\n",
      "66954\n",
      "66955\n",
      "66956\n",
      "66957\n",
      "66958\n",
      "66959\n",
      "66960\n",
      "66961\n",
      "66962\n",
      "66963\n",
      "66964\n",
      "66965\n",
      "66966\n",
      "66967\n",
      "66968\n",
      "66969\n",
      "66970\n",
      "66971\n",
      "66972\n",
      "66973\n",
      "66974\n",
      "66975\n",
      "66976\n",
      "66977\n",
      "66978\n",
      "66979\n",
      "66980\n",
      "66981\n",
      "66982\n",
      "66983\n",
      "66984\n",
      "66985\n",
      "66986\n",
      "66987\n",
      "66988\n",
      "66989\n",
      "66990\n",
      "66991\n",
      "66992\n",
      "66993\n",
      "66994\n",
      "66995\n",
      "66996\n",
      "66997\n",
      "66998\n",
      "66999\n",
      "67000\n",
      "67001\n",
      "67002\n",
      "67003\n",
      "67004\n",
      "67005\n",
      "67006\n",
      "67007\n",
      "67008\n",
      "67009\n",
      "67010\n",
      "67011\n",
      "67012\n",
      "67013\n",
      "67014\n",
      "67015\n",
      "67016\n",
      "67017\n",
      "67018\n",
      "67019\n",
      "67020\n",
      "67021\n",
      "67022\n",
      "67023\n",
      "67024\n",
      "67025\n",
      "67026\n",
      "67027\n",
      "67028\n",
      "67029\n",
      "67030\n",
      "67031\n",
      "67032\n",
      "67033\n",
      "67034\n",
      "67035\n",
      "67036\n",
      "67037\n",
      "67038\n",
      "67039\n",
      "67040\n",
      "67041\n",
      "67042\n",
      "67043\n",
      "67044\n",
      "67045\n",
      "67046\n",
      "67047\n",
      "67048\n",
      "67049\n",
      "67050\n",
      "67051\n",
      "67052\n",
      "67053\n",
      "67054\n",
      "67055\n",
      "67056\n",
      "67057\n",
      "67058\n",
      "67059\n",
      "67060\n",
      "67061\n",
      "67062\n",
      "67063\n",
      "67064\n",
      "67065\n",
      "67066\n",
      "67067\n",
      "67068\n",
      "67069\n",
      "67070\n",
      "67071\n",
      "67072\n",
      "67073\n",
      "67074\n",
      "67075\n",
      "67076\n",
      "67077\n",
      "67078\n",
      "67079\n",
      "67080\n",
      "67081\n",
      "67082\n",
      "67083\n",
      "67084\n",
      "67085\n",
      "67086\n",
      "67087\n",
      "67088\n",
      "67089\n",
      "67090\n",
      "67091\n",
      "67092\n",
      "67093\n",
      "67094\n",
      "67095\n",
      "67096\n",
      "67097\n",
      "67098\n",
      "67099\n",
      "67100\n",
      "67101\n",
      "67102\n",
      "67103\n",
      "67104\n",
      "67105\n",
      "67106\n",
      "67107\n",
      "67108\n",
      "67109\n",
      "67110\n",
      "67111\n",
      "67112\n",
      "67113\n",
      "67114\n",
      "67115\n",
      "67116\n",
      "67117\n",
      "67118\n",
      "67119\n",
      "67120\n",
      "67121\n",
      "67122\n",
      "67123\n",
      "67124\n",
      "67125\n",
      "67126\n",
      "67127\n",
      "67128\n",
      "67129\n",
      "67130\n",
      "67131\n",
      "67132\n",
      "67133\n",
      "67134\n",
      "67135\n",
      "67136\n",
      "67137\n",
      "67138\n",
      "67139\n",
      "67140\n",
      "67141\n",
      "67142\n",
      "67143\n",
      "67144\n",
      "67145\n",
      "67146\n",
      "67147\n",
      "67148\n",
      "67149\n",
      "67150\n",
      "67151\n",
      "67152\n",
      "67153\n",
      "67154\n",
      "67155\n",
      "67156\n",
      "67157\n",
      "67158\n",
      "67159\n",
      "67160\n",
      "67161\n",
      "67162\n",
      "67163\n",
      "67164\n",
      "67165\n",
      "67166\n",
      "67167\n",
      "67168\n",
      "67169\n",
      "67170\n",
      "67171\n",
      "67172\n",
      "67173\n",
      "67174\n",
      "67175\n",
      "67176\n",
      "67177\n",
      "67178\n",
      "67179\n",
      "67180\n",
      "67181\n",
      "67182\n",
      "67183\n",
      "67184\n",
      "67185\n",
      "67186\n",
      "67187\n",
      "67188\n",
      "67189\n",
      "67190\n",
      "67191\n",
      "67192\n",
      "67193\n",
      "67194\n",
      "67195\n",
      "67196\n",
      "67197\n",
      "67198\n",
      "67199\n",
      "67200\n",
      "67201\n",
      "67202\n",
      "67203\n",
      "67204\n",
      "67205\n",
      "67206\n",
      "67207\n",
      "67208\n",
      "67209\n",
      "67210\n",
      "67211\n",
      "67212\n",
      "67213\n",
      "67214\n",
      "67215\n",
      "67216\n",
      "67217\n",
      "67218\n",
      "67219\n",
      "67220\n",
      "67221\n",
      "67222\n",
      "67223\n",
      "67224\n",
      "67225\n",
      "67226\n",
      "67227\n",
      "67228\n",
      "67229\n",
      "67230\n",
      "67231\n",
      "67232\n",
      "67233\n",
      "67234\n",
      "67235\n",
      "67236\n",
      "67237\n",
      "67238\n",
      "67239\n",
      "67240\n",
      "67241\n",
      "67242\n",
      "67243\n",
      "67244\n",
      "67245\n",
      "67246\n",
      "67247\n",
      "67248\n",
      "67249\n",
      "67250\n",
      "67251\n",
      "67252\n",
      "67253\n",
      "67254\n",
      "67255\n",
      "67256\n",
      "67257\n",
      "67258\n",
      "67259\n",
      "67260\n",
      "67261\n",
      "67262\n",
      "67263\n",
      "67264\n",
      "67265\n",
      "67266\n",
      "67267\n",
      "67268\n",
      "67269\n",
      "67270\n",
      "67271\n",
      "67272\n",
      "67273\n",
      "67274\n",
      "67275\n",
      "67276\n",
      "67277\n",
      "67278\n",
      "67279\n",
      "67280\n",
      "67281\n",
      "67282\n",
      "67283\n",
      "67284\n",
      "67285\n",
      "67286\n",
      "67287\n",
      "67288\n",
      "67289\n",
      "67290\n",
      "67291\n",
      "67292\n",
      "67293\n",
      "67294\n",
      "67295\n",
      "67296\n",
      "67297\n",
      "67298\n",
      "67299\n",
      "67300\n",
      "67301\n",
      "67302\n",
      "67303\n",
      "67304\n",
      "67305\n",
      "67306\n",
      "67307\n",
      "67308\n",
      "67309\n",
      "67310\n",
      "67311\n",
      "67312\n",
      "67313\n",
      "67314\n",
      "67315\n",
      "67316\n",
      "67317\n",
      "67318\n",
      "67319\n",
      "67320\n",
      "67321\n",
      "67322\n",
      "67323\n",
      "67324\n",
      "67325\n",
      "67326\n",
      "67327\n",
      "67328\n",
      "67329\n",
      "67330\n",
      "67331\n",
      "67332\n",
      "67333\n",
      "67334\n",
      "67335\n",
      "67336\n",
      "67337\n",
      "67338\n",
      "67339\n",
      "67340\n",
      "67341\n",
      "67342\n",
      "67343\n",
      "67344\n",
      "67345\n",
      "67346\n",
      "67347\n",
      "67348\n",
      "67349\n",
      "67350\n",
      "67351\n",
      "67352\n",
      "67353\n",
      "67354\n",
      "67355\n",
      "67356\n",
      "67357\n",
      "67358\n",
      "67359\n",
      "67360\n",
      "67361\n",
      "67362\n",
      "67363\n",
      "67364\n",
      "67365\n",
      "67366\n",
      "67367\n",
      "67368\n",
      "67369\n",
      "67370\n",
      "67371\n",
      "67372\n",
      "67373\n",
      "67374\n",
      "67375\n",
      "67376\n",
      "67377\n",
      "67378\n",
      "67379\n",
      "67380\n",
      "67381\n",
      "67382\n",
      "67383\n",
      "67384\n",
      "67385\n",
      "67386\n",
      "67387\n",
      "67388\n",
      "67389\n",
      "67390\n",
      "67391\n",
      "67392\n",
      "67393\n",
      "67394\n",
      "67395\n",
      "67396\n",
      "67397\n",
      "67398\n",
      "67399\n",
      "67400\n",
      "67401\n",
      "67402\n",
      "67403\n",
      "67404\n",
      "67405\n",
      "67406\n",
      "67407\n",
      "67408\n",
      "67409\n",
      "67410\n",
      "67411\n",
      "67412\n",
      "67413\n",
      "67414\n",
      "67415\n",
      "67416\n",
      "67417\n",
      "67418\n",
      "67419\n",
      "67420\n",
      "67421\n",
      "67422\n",
      "67423\n",
      "67424\n",
      "67425\n",
      "67426\n",
      "67427\n",
      "67428\n",
      "67429\n",
      "67430\n",
      "67431\n",
      "67432\n",
      "67433\n",
      "67434\n",
      "67435\n",
      "67436\n",
      "67437\n",
      "67438\n",
      "67439\n",
      "67440\n",
      "67441\n",
      "67442\n",
      "67443\n",
      "67444\n",
      "67445\n",
      "67446\n",
      "67447\n",
      "67448\n",
      "67449\n",
      "67450\n",
      "67451\n",
      "67452\n",
      "67453\n",
      "67454\n",
      "67455\n",
      "67456\n",
      "67457\n",
      "67458\n",
      "67459\n",
      "67460\n",
      "67461\n",
      "67462\n",
      "67463\n",
      "67464\n",
      "67465\n",
      "67466\n",
      "67467\n",
      "67468\n",
      "67469\n",
      "67470\n",
      "67471\n",
      "67472\n",
      "67473\n",
      "67474\n",
      "67475\n",
      "67476\n",
      "67477\n",
      "67478\n",
      "67479\n",
      "67480\n",
      "67481\n",
      "67482\n",
      "67483\n",
      "67484\n",
      "67485\n",
      "67486\n",
      "67487\n",
      "67488\n",
      "67489\n",
      "67490\n",
      "67491\n",
      "67492\n",
      "67493\n",
      "67494\n",
      "67495\n",
      "67496\n",
      "67497\n",
      "67498\n",
      "67499\n",
      "67500\n",
      "67501\n",
      "67502\n",
      "67503\n",
      "67504\n",
      "67505\n",
      "67506\n",
      "67507\n",
      "67508\n",
      "67509\n",
      "67510\n",
      "67511\n",
      "67512\n",
      "67513\n",
      "67514\n",
      "67515\n",
      "67516\n",
      "67517\n",
      "67518\n",
      "67519\n",
      "67520\n",
      "67521\n",
      "67522\n",
      "67523\n",
      "67524\n",
      "67525\n",
      "67526\n",
      "67527\n",
      "67528\n",
      "67529\n",
      "67530\n",
      "67531\n",
      "67532\n",
      "67533\n",
      "67534\n",
      "67535\n",
      "67536\n",
      "67537\n",
      "67538\n",
      "67539\n",
      "67540\n",
      "67541\n",
      "67542\n",
      "67543\n",
      "67544\n",
      "67545\n",
      "67546\n",
      "67547\n",
      "67548\n",
      "67549\n",
      "67550\n",
      "67551\n",
      "67552\n",
      "67553\n",
      "67554\n",
      "67555\n",
      "67556\n",
      "67557\n",
      "67558\n",
      "67559\n",
      "67560\n",
      "67561\n",
      "67562\n",
      "67563\n",
      "67564\n",
      "67565\n",
      "67566\n",
      "67567\n",
      "67568\n",
      "67569\n",
      "67570\n",
      "67571\n",
      "67572\n",
      "67573\n",
      "67574\n",
      "67575\n",
      "67576\n",
      "67577\n",
      "67578\n",
      "67579\n",
      "67580\n",
      "67581\n",
      "67582\n",
      "67583\n",
      "67584\n",
      "67585\n",
      "67586\n",
      "67587\n",
      "67588\n",
      "67589\n",
      "67590\n",
      "67591\n",
      "67592\n",
      "67593\n",
      "67594\n",
      "67595\n",
      "67596\n",
      "67597\n",
      "67598\n",
      "67599\n",
      "67600\n",
      "67601\n",
      "67602\n",
      "67603\n",
      "67604\n",
      "67605\n",
      "67606\n",
      "67607\n",
      "67608\n",
      "67609\n",
      "67610\n",
      "67611\n",
      "67612\n",
      "67613\n",
      "67614\n",
      "67615\n",
      "67616\n",
      "67617\n",
      "67618\n",
      "67619\n",
      "67620\n",
      "67621\n",
      "67622\n",
      "67623\n",
      "67624\n",
      "67625\n",
      "67626\n",
      "67627\n",
      "67628\n",
      "67629\n",
      "67630\n",
      "67631\n",
      "67632\n",
      "67633\n",
      "67634\n",
      "67635\n",
      "67636\n",
      "67637\n",
      "67638\n",
      "67639\n",
      "67640\n",
      "67641\n",
      "67642\n",
      "67643\n",
      "67644\n",
      "67645\n",
      "67646\n",
      "67647\n",
      "67648\n",
      "67649\n",
      "67650\n",
      "67651\n",
      "67652\n",
      "67653\n",
      "67654\n",
      "67655\n",
      "67656\n",
      "67657\n",
      "67658\n",
      "67659\n",
      "67660\n",
      "67661\n",
      "67662\n",
      "67663\n",
      "67664\n",
      "67665\n",
      "67666\n",
      "67667\n",
      "67668\n",
      "67669\n",
      "67670\n",
      "67671\n",
      "67672\n",
      "67673\n",
      "67674\n",
      "67675\n",
      "67676\n",
      "67677\n",
      "67678\n",
      "67679\n",
      "67680\n",
      "67681\n",
      "67682\n",
      "67683\n",
      "67684\n",
      "67685\n",
      "67686\n",
      "67687\n",
      "67688\n",
      "67689\n",
      "67690\n",
      "67691\n",
      "67692\n",
      "67693\n",
      "67694\n",
      "67695\n",
      "67696\n",
      "67697\n",
      "67698\n",
      "67699\n",
      "67700\n",
      "67701\n",
      "67702\n",
      "67703\n",
      "67704\n",
      "67705\n",
      "67706\n",
      "67707\n",
      "67708\n",
      "67709\n",
      "67710\n",
      "67711\n",
      "67712\n",
      "67713\n",
      "67714\n",
      "67715\n",
      "67716\n",
      "67717\n",
      "67718\n",
      "67719\n",
      "67720\n",
      "67721\n",
      "67722\n",
      "67723\n",
      "67724\n",
      "67725\n",
      "67726\n",
      "67727\n",
      "67728\n",
      "67729\n",
      "67730\n",
      "67731\n",
      "67732\n",
      "67733\n",
      "67734\n",
      "67735\n",
      "67736\n",
      "67737\n",
      "67738\n",
      "67739\n",
      "67740\n",
      "67741\n",
      "67742\n",
      "67743\n",
      "67744\n",
      "67745\n",
      "67746\n",
      "67747\n",
      "67748\n",
      "67749\n",
      "67750\n",
      "67751\n",
      "67752\n",
      "67753\n",
      "67754\n",
      "67755\n",
      "67756\n",
      "67757\n",
      "67758\n",
      "67759\n",
      "67760\n",
      "67761\n",
      "67762\n",
      "67763\n",
      "67764\n",
      "67765\n",
      "67766\n",
      "67767\n",
      "67768\n",
      "67769\n",
      "67770\n",
      "67771\n",
      "67772\n",
      "67773\n",
      "67774\n",
      "67775\n",
      "67776\n",
      "67777\n",
      "67778\n",
      "67779\n",
      "67780\n",
      "67781\n",
      "67782\n",
      "67783\n",
      "67784\n",
      "67785\n",
      "67786\n",
      "67787\n",
      "67788\n",
      "67789\n",
      "67790\n",
      "67791\n",
      "67792\n",
      "67793\n",
      "67794\n",
      "67795\n",
      "67796\n",
      "67797\n",
      "67798\n",
      "67799\n",
      "67800\n",
      "67801\n",
      "67802\n",
      "67803\n",
      "67804\n",
      "67805\n",
      "67806\n",
      "67807\n",
      "67808\n",
      "67809\n",
      "67810\n",
      "67811\n",
      "67812\n",
      "67813\n",
      "67814\n",
      "67815\n",
      "67816\n",
      "67817\n",
      "67818\n",
      "67819\n",
      "67820\n",
      "67821\n",
      "67822\n",
      "67823\n",
      "67824\n",
      "67825\n",
      "67826\n",
      "67827\n",
      "67828\n",
      "67829\n",
      "67830\n",
      "67831\n",
      "67832\n",
      "67833\n",
      "67834\n",
      "67835\n",
      "67836\n",
      "67837\n",
      "67838\n",
      "67839\n",
      "67840\n",
      "67841\n",
      "67842\n",
      "67843\n",
      "67844\n",
      "67845\n",
      "67846\n",
      "67847\n",
      "67848\n",
      "67849\n",
      "67850\n",
      "67851\n",
      "67852\n",
      "67853\n",
      "67854\n",
      "67855\n",
      "67856\n",
      "67857\n",
      "67858\n",
      "67859\n",
      "67860\n",
      "67861\n",
      "67862\n",
      "67863\n",
      "67864\n",
      "67865\n",
      "67866\n",
      "67867\n",
      "67868\n",
      "67869\n",
      "67870\n",
      "67871\n",
      "67872\n",
      "67873\n",
      "67874\n",
      "67875\n",
      "67876\n",
      "67877\n",
      "67878\n",
      "67879\n",
      "67880\n",
      "67881\n",
      "67882\n",
      "67883\n",
      "67884\n",
      "67885\n",
      "67886\n",
      "67887\n",
      "67888\n",
      "67889\n",
      "67890\n",
      "67891\n",
      "67892\n",
      "67893\n",
      "67894\n",
      "67895\n",
      "67896\n",
      "67897\n",
      "67898\n",
      "67899\n",
      "67900\n",
      "67901\n",
      "67902\n",
      "67903\n",
      "67904\n",
      "67905\n",
      "67906\n",
      "67907\n",
      "67908\n",
      "67909\n",
      "67910\n",
      "67911\n",
      "67912\n",
      "67913\n",
      "67914\n",
      "67915\n",
      "67916\n",
      "67917\n",
      "67918\n",
      "67919\n",
      "67920\n",
      "67921\n",
      "67922\n",
      "67923\n",
      "67924\n",
      "67925\n",
      "67926\n",
      "67927\n",
      "67928\n",
      "67929\n",
      "67930\n",
      "67931\n",
      "67932\n",
      "67933\n",
      "67934\n",
      "67935\n",
      "67936\n",
      "67937\n",
      "67938\n",
      "67939\n",
      "67940\n",
      "67941\n",
      "67942\n",
      "67943\n",
      "67944\n",
      "67945\n",
      "67946\n",
      "67947\n",
      "67948\n",
      "67949\n",
      "67950\n",
      "67951\n",
      "67952\n",
      "67953\n",
      "67954\n",
      "67955\n",
      "67956\n",
      "67957\n",
      "67958\n",
      "67959\n",
      "67960\n",
      "67961\n",
      "67962\n",
      "67963\n",
      "67964\n",
      "67965\n",
      "67966\n",
      "67967\n",
      "67968\n",
      "67969\n",
      "67970\n",
      "67971\n",
      "67972\n",
      "67973\n",
      "67974\n",
      "67975\n",
      "67976\n",
      "67977\n",
      "67978\n",
      "67979\n",
      "67980\n",
      "67981\n",
      "67982\n",
      "67983\n",
      "67984\n",
      "67985\n",
      "67986\n",
      "67987\n",
      "67988\n",
      "67989\n",
      "67990\n",
      "67991\n",
      "67992\n",
      "67993\n",
      "67994\n",
      "67995\n",
      "67996\n",
      "67997\n",
      "67998\n",
      "67999\n",
      "68000\n",
      "68001\n",
      "68002\n",
      "68003\n",
      "68004\n",
      "68005\n",
      "68006\n",
      "68007\n",
      "68008\n",
      "68009\n",
      "68010\n",
      "68011\n",
      "68012\n",
      "68013\n",
      "68014\n",
      "68015\n",
      "68016\n",
      "68017\n",
      "68018\n",
      "68019\n",
      "68020\n",
      "68021\n",
      "68022\n",
      "68023\n",
      "68024\n",
      "68025\n",
      "68026\n",
      "68027\n",
      "68028\n",
      "68029\n",
      "68030\n",
      "68031\n",
      "68032\n",
      "68033\n",
      "68034\n",
      "68035\n",
      "68036\n",
      "68037\n",
      "68038\n",
      "68039\n",
      "68040\n",
      "68041\n",
      "68042\n",
      "68043\n",
      "68044\n",
      "68045\n",
      "68046\n",
      "68047\n",
      "68048\n",
      "68049\n",
      "68050\n",
      "68051\n",
      "68052\n",
      "68053\n",
      "68054\n",
      "68055\n",
      "68056\n",
      "68057\n",
      "68058\n",
      "68059\n",
      "68060\n",
      "68061\n",
      "68062\n",
      "68063\n",
      "68064\n",
      "68065\n",
      "68066\n",
      "68067\n",
      "68068\n",
      "68069\n",
      "68070\n",
      "68071\n",
      "68072\n",
      "68073\n",
      "68074\n",
      "68075\n",
      "68076\n",
      "68077\n",
      "68078\n",
      "68079\n",
      "68080\n",
      "68081\n",
      "68082\n",
      "68083\n",
      "68084\n",
      "68085\n",
      "68086\n",
      "68087\n",
      "68088\n",
      "68089\n",
      "68090\n",
      "68091\n",
      "68092\n",
      "68093\n",
      "68094\n",
      "68095\n",
      "68096\n",
      "68097\n",
      "68098\n",
      "68099\n",
      "68100\n",
      "68101\n",
      "68102\n",
      "68103\n",
      "68104\n",
      "68105\n",
      "68106\n",
      "68107\n",
      "68108\n",
      "68109\n",
      "68110\n",
      "68111\n",
      "68112\n",
      "68113\n",
      "68114\n",
      "68115\n",
      "68116\n",
      "68117\n",
      "68118\n",
      "68119\n",
      "68120\n",
      "68121\n",
      "68122\n",
      "68123\n",
      "68124\n",
      "68125\n",
      "68126\n",
      "68127\n",
      "68128\n",
      "68129\n",
      "68130\n",
      "68131\n",
      "68132\n",
      "68133\n",
      "68134\n",
      "68135\n",
      "68136\n",
      "68137\n",
      "68138\n",
      "68139\n",
      "68140\n",
      "68141\n",
      "68142\n",
      "68143\n",
      "68144\n",
      "68145\n",
      "68146\n",
      "68147\n",
      "68148\n",
      "68149\n",
      "68150\n",
      "68151\n",
      "68152\n",
      "68153\n",
      "68154\n",
      "68155\n",
      "68156\n",
      "68157\n",
      "68158\n",
      "68159\n",
      "68160\n",
      "68161\n",
      "68162\n",
      "68163\n",
      "68164\n",
      "68165\n",
      "68166\n",
      "68167\n",
      "68168\n",
      "68169\n",
      "68170\n",
      "68171\n",
      "68172\n",
      "68173\n",
      "68174\n",
      "68175\n",
      "68176\n",
      "68177\n",
      "68178\n",
      "68179\n",
      "68180\n",
      "68181\n",
      "68182\n",
      "68183\n",
      "68184\n",
      "68185\n",
      "68186\n",
      "68187\n",
      "68188\n",
      "68189\n",
      "68190\n",
      "68191\n",
      "68192\n",
      "68193\n",
      "68194\n",
      "68195\n",
      "68196\n",
      "68197\n",
      "68198\n",
      "68199\n",
      "68200\n",
      "68201\n",
      "68202\n",
      "68203\n",
      "68204\n",
      "68205\n",
      "68206\n",
      "68207\n",
      "68208\n",
      "68209\n",
      "68210\n",
      "68211\n",
      "68212\n",
      "68213\n",
      "68214\n",
      "68215\n",
      "68216\n",
      "68217\n",
      "68218\n",
      "68219\n",
      "68220\n",
      "68221\n",
      "68222\n",
      "68223\n",
      "68224\n",
      "68225\n",
      "68226\n",
      "68227\n",
      "68228\n",
      "68229\n",
      "68230\n",
      "68231\n",
      "68232\n",
      "68233\n",
      "68234\n",
      "68235\n",
      "68236\n",
      "68237\n",
      "68238\n",
      "68239\n",
      "68240\n",
      "68241\n",
      "68242\n",
      "68243\n",
      "68244\n",
      "68245\n",
      "68246\n",
      "68247\n",
      "68248\n",
      "68249\n",
      "68250\n",
      "68251\n",
      "68252\n",
      "68253\n",
      "68254\n",
      "68255\n",
      "68256\n",
      "68257\n",
      "68258\n",
      "68259\n",
      "68260\n",
      "68261\n",
      "68262\n",
      "68263\n",
      "68264\n",
      "68265\n",
      "68266\n",
      "68267\n",
      "68268\n",
      "68269\n",
      "68270\n",
      "68271\n",
      "68272\n",
      "68273\n",
      "68274\n",
      "68275\n",
      "68276\n",
      "68277\n",
      "68278\n",
      "68279\n",
      "68280\n",
      "68281\n",
      "68282\n",
      "68283\n",
      "68284\n",
      "68285\n",
      "68286\n",
      "68287\n",
      "68288\n",
      "68289\n",
      "68290\n",
      "68291\n",
      "68292\n",
      "68293\n",
      "68294\n",
      "68295\n",
      "68296\n",
      "68297\n",
      "68298\n",
      "68299\n",
      "68300\n",
      "68301\n",
      "68302\n",
      "68303\n",
      "68304\n",
      "68305\n",
      "68306\n",
      "68307\n",
      "68308\n",
      "68309\n",
      "68310\n",
      "68311\n",
      "68312\n",
      "68313\n",
      "68314\n",
      "68315\n",
      "68316\n",
      "68317\n",
      "68318\n",
      "68319\n",
      "68320\n",
      "68321\n",
      "68322\n",
      "68323\n",
      "68324\n",
      "68325\n",
      "68326\n",
      "68327\n",
      "68328\n",
      "68329\n",
      "68330\n",
      "68331\n",
      "68332\n",
      "68333\n",
      "68334\n",
      "68335\n",
      "68336\n",
      "68337\n",
      "68338\n",
      "68339\n",
      "68340\n",
      "68341\n",
      "68342\n",
      "68343\n",
      "68344\n",
      "68345\n",
      "68346\n",
      "68347\n",
      "68348\n",
      "68349\n",
      "68350\n",
      "68351\n",
      "68352\n",
      "68353\n",
      "68354\n",
      "68355\n",
      "68356\n",
      "68357\n",
      "68358\n",
      "68359\n",
      "68360\n",
      "68361\n",
      "68362\n",
      "68363\n",
      "68364\n",
      "68365\n",
      "68366\n",
      "68367\n",
      "68368\n",
      "68369\n",
      "68370\n",
      "68371\n",
      "68372\n",
      "68373\n",
      "68374\n",
      "68375\n",
      "68376\n",
      "68377\n",
      "68378\n",
      "68379\n",
      "68380\n",
      "68381\n",
      "68382\n",
      "68383\n",
      "68384\n",
      "68385\n",
      "68386\n",
      "68387\n",
      "68388\n",
      "68389\n",
      "68390\n",
      "68391\n",
      "68392\n",
      "68393\n",
      "68394\n",
      "68395\n",
      "68396\n",
      "68397\n",
      "68398\n",
      "68399\n",
      "68400\n",
      "68401\n",
      "68402\n",
      "68403\n",
      "68404\n",
      "68405\n",
      "68406\n",
      "68407\n",
      "68408\n",
      "68409\n",
      "68410\n",
      "68411\n",
      "68412\n",
      "68413\n",
      "68414\n",
      "68415\n",
      "68416\n",
      "68417\n",
      "68418\n",
      "68419\n",
      "68420\n",
      "68421\n",
      "68422\n",
      "68423\n",
      "68424\n",
      "68425\n",
      "68426\n",
      "68427\n",
      "68428\n",
      "68429\n",
      "68430\n",
      "68431\n",
      "68432\n",
      "68433\n",
      "68434\n",
      "68435\n",
      "68436\n",
      "68437\n",
      "68438\n",
      "68439\n",
      "68440\n",
      "68441\n",
      "68442\n",
      "68443\n",
      "68444\n",
      "68445\n",
      "68446\n",
      "68447\n",
      "68448\n",
      "68449\n",
      "68450\n",
      "68451\n",
      "68452\n",
      "68453\n",
      "68454\n",
      "68455\n",
      "68456\n",
      "68457\n",
      "68458\n",
      "68459\n",
      "68460\n",
      "68461\n",
      "68462\n",
      "68463\n",
      "68464\n",
      "68465\n",
      "68466\n",
      "68467\n",
      "68468\n",
      "68469\n",
      "68470\n",
      "68471\n",
      "68472\n",
      "68473\n",
      "68474\n",
      "68475\n",
      "68476\n",
      "68477\n",
      "68478\n",
      "68479\n",
      "68480\n",
      "68481\n",
      "68482\n",
      "68483\n",
      "68484\n",
      "68485\n",
      "68486\n",
      "68487\n",
      "68488\n",
      "68489\n",
      "68490\n",
      "68491\n",
      "68492\n",
      "68493\n",
      "68494\n",
      "68495\n",
      "68496\n",
      "68497\n",
      "68498\n",
      "68499\n",
      "68500\n",
      "68501\n",
      "68502\n",
      "68503\n",
      "68504\n",
      "68505\n",
      "68506\n",
      "68507\n",
      "68508\n",
      "68509\n",
      "68510\n",
      "68511\n",
      "68512\n",
      "68513\n",
      "68514\n",
      "68515\n",
      "68516\n",
      "68517\n",
      "68518\n",
      "68519\n",
      "68520\n",
      "68521\n",
      "68522\n",
      "68523\n",
      "68524\n",
      "68525\n",
      "68526\n",
      "68527\n",
      "68528\n",
      "68529\n",
      "68530\n",
      "68531\n",
      "68532\n",
      "68533\n",
      "68534\n",
      "68535\n",
      "68536\n",
      "68537\n",
      "68538\n",
      "68539\n",
      "68540\n",
      "68541\n",
      "68542\n",
      "68543\n",
      "68544\n",
      "68545\n",
      "68546\n",
      "68547\n",
      "68548\n",
      "68549\n",
      "68550\n",
      "68551\n",
      "68552\n",
      "68553\n",
      "68554\n",
      "68555\n",
      "68556\n",
      "68557\n",
      "68558\n",
      "68559\n",
      "68560\n",
      "68561\n",
      "68562\n",
      "68563\n",
      "68564\n",
      "68565\n",
      "68566\n",
      "68567\n",
      "68568\n",
      "68569\n",
      "68570\n",
      "68571\n",
      "68572\n",
      "68573\n",
      "68574\n",
      "68575\n",
      "68576\n",
      "68577\n",
      "68578\n",
      "68579\n",
      "68580\n",
      "68581\n",
      "68582\n",
      "68583\n",
      "68584\n",
      "68585\n",
      "68586\n",
      "68587\n",
      "68588\n",
      "68589\n",
      "68590\n",
      "68591\n",
      "68592\n",
      "68593\n",
      "68594\n",
      "68595\n",
      "68596\n",
      "68597\n",
      "68598\n",
      "68599\n",
      "68600\n",
      "68601\n",
      "68602\n",
      "68603\n",
      "68604\n",
      "68605\n",
      "68606\n",
      "68607\n",
      "68608\n",
      "68609\n",
      "68610\n",
      "68611\n",
      "68612\n",
      "68613\n",
      "68614\n",
      "68615\n",
      "68616\n",
      "68617\n",
      "68618\n",
      "68619\n",
      "68620\n",
      "68621\n",
      "68622\n",
      "68623\n",
      "68624\n",
      "68625\n",
      "68626\n",
      "68627\n",
      "68628\n",
      "68629\n",
      "68630\n",
      "68631\n",
      "68632\n",
      "68633\n",
      "68634\n",
      "68635\n",
      "68636\n",
      "68637\n",
      "68638\n",
      "68639\n",
      "68640\n",
      "68641\n",
      "68642\n",
      "68643\n",
      "68644\n",
      "68645\n",
      "68646\n",
      "68647\n",
      "68648\n",
      "68649\n",
      "68650\n",
      "68651\n",
      "68652\n",
      "68653\n",
      "68654\n",
      "68655\n",
      "68656\n",
      "68657\n",
      "68658\n",
      "68659\n",
      "68660\n",
      "68661\n",
      "68662\n",
      "68663\n",
      "68664\n",
      "68665\n",
      "68666\n",
      "68667\n",
      "68668\n",
      "68669\n",
      "68670\n",
      "68671\n",
      "68672\n",
      "68673\n",
      "68674\n",
      "68675\n",
      "68676\n",
      "68677\n",
      "68678\n",
      "68679\n",
      "68680\n",
      "68681\n",
      "68682\n",
      "68683\n",
      "68684\n",
      "68685\n",
      "68686\n",
      "68687\n",
      "68688\n",
      "68689\n",
      "68690\n",
      "68691\n",
      "68692\n",
      "68693\n",
      "68694\n",
      "68695\n",
      "68696\n",
      "68697\n",
      "68698\n",
      "68699\n",
      "68700\n",
      "68701\n",
      "68702\n",
      "68703\n",
      "68704\n",
      "68705\n",
      "68706\n",
      "68707\n",
      "68708\n",
      "68709\n",
      "68710\n",
      "68711\n",
      "68712\n",
      "68713\n",
      "68714\n",
      "68715\n",
      "68716\n",
      "68717\n",
      "68718\n",
      "68719\n",
      "68720\n",
      "68721\n",
      "68722\n",
      "68723\n",
      "68724\n",
      "68725\n",
      "68726\n",
      "68727\n",
      "68728\n",
      "68729\n",
      "68730\n",
      "68731\n",
      "68732\n",
      "68733\n",
      "68734\n",
      "68735\n",
      "68736\n",
      "68737\n",
      "68738\n",
      "68739\n",
      "68740\n",
      "68741\n",
      "68742\n",
      "68743\n",
      "68744\n",
      "68745\n",
      "68746\n",
      "68747\n",
      "68748\n",
      "68749\n",
      "68750\n",
      "68751\n",
      "68752\n",
      "68753\n",
      "68754\n",
      "68755\n",
      "68756\n",
      "68757\n",
      "68758\n",
      "68759\n",
      "68760\n",
      "68761\n",
      "68762\n",
      "68763\n",
      "68764\n",
      "68765\n",
      "68766\n",
      "68767\n",
      "68768\n",
      "68769\n",
      "68770\n",
      "68771\n",
      "68772\n",
      "68773\n",
      "68774\n",
      "68775\n",
      "68776\n",
      "68777\n",
      "68778\n",
      "68779\n",
      "68780\n",
      "68781\n",
      "68782\n",
      "68783\n",
      "68784\n",
      "68785\n",
      "68786\n",
      "68787\n",
      "68788\n",
      "68789\n",
      "68790\n",
      "68791\n",
      "68792\n",
      "68793\n",
      "68794\n",
      "68795\n",
      "68796\n",
      "68797\n",
      "68798\n",
      "68799\n",
      "68800\n",
      "68801\n",
      "68802\n",
      "68803\n",
      "68804\n",
      "68805\n",
      "68806\n",
      "68807\n",
      "68808\n",
      "68809\n",
      "68810\n",
      "68811\n",
      "68812\n",
      "68813\n",
      "68814\n",
      "68815\n",
      "68816\n",
      "68817\n",
      "68818\n",
      "68819\n",
      "68820\n",
      "68821\n",
      "68822\n",
      "68823\n",
      "68824\n",
      "68825\n",
      "68826\n",
      "68827\n",
      "68828\n",
      "68829\n",
      "68830\n",
      "68831\n",
      "68832\n",
      "68833\n",
      "68834\n",
      "68835\n",
      "68836\n",
      "68837\n",
      "68838\n",
      "68839\n",
      "68840\n",
      "68841\n",
      "68842\n",
      "68843\n",
      "68844\n",
      "68845\n",
      "68846\n",
      "68847\n",
      "68848\n",
      "68849\n",
      "68850\n",
      "68851\n",
      "68852\n",
      "68853\n",
      "68854\n",
      "68855\n",
      "68856\n",
      "68857\n",
      "68858\n",
      "68859\n",
      "68860\n",
      "68861\n",
      "68862\n",
      "68863\n",
      "68864\n",
      "68865\n",
      "68866\n",
      "68867\n",
      "68868\n",
      "68869\n",
      "68870\n",
      "68871\n",
      "68872\n",
      "68873\n",
      "68874\n",
      "68875\n",
      "68876\n",
      "68877\n",
      "68878\n",
      "68879\n",
      "68880\n",
      "68881\n",
      "68882\n",
      "68883\n",
      "68884\n",
      "68885\n",
      "68886\n",
      "68887\n",
      "68888\n",
      "68889\n",
      "68890\n",
      "68891\n",
      "68892\n",
      "68893\n",
      "68894\n",
      "68895\n",
      "68896\n",
      "68897\n",
      "68898\n",
      "68899\n",
      "68900\n",
      "68901\n",
      "68902\n",
      "68903\n",
      "68904\n",
      "68905\n",
      "68906\n",
      "68907\n",
      "68908\n",
      "68909\n",
      "68910\n",
      "68911\n",
      "68912\n",
      "68913\n",
      "68914\n",
      "68915\n",
      "68916\n",
      "68917\n",
      "68918\n",
      "68919\n",
      "68920\n",
      "68921\n",
      "68922\n",
      "68923\n",
      "68924\n",
      "68925\n",
      "68926\n",
      "68927\n",
      "68928\n",
      "68929\n",
      "68930\n",
      "68931\n",
      "68932\n",
      "68933\n",
      "68934\n",
      "68935\n",
      "68936\n",
      "68937\n",
      "68938\n",
      "68939\n",
      "68940\n",
      "68941\n",
      "68942\n",
      "68943\n",
      "68944\n",
      "68945\n",
      "68946\n",
      "68947\n",
      "68948\n",
      "68949\n",
      "68950\n",
      "68951\n",
      "68952\n",
      "68953\n",
      "68954\n",
      "68955\n",
      "68956\n",
      "68957\n",
      "68958\n",
      "68959\n",
      "68960\n",
      "68961\n",
      "68962\n",
      "68963\n",
      "68964\n",
      "68965\n",
      "68966\n",
      "68967\n",
      "68968\n",
      "68969\n",
      "68970\n",
      "68971\n",
      "68972\n",
      "68973\n",
      "68974\n",
      "68975\n",
      "68976\n",
      "68977\n",
      "68978\n",
      "68979\n",
      "68980\n",
      "68981\n",
      "68982\n",
      "68983\n",
      "68984\n",
      "68985\n",
      "68986\n",
      "68987\n",
      "68988\n",
      "68989\n",
      "68990\n",
      "68991\n",
      "68992\n",
      "68993\n",
      "68994\n",
      "68995\n",
      "68996\n",
      "68997\n",
      "68998\n",
      "68999\n",
      "69000\n",
      "69001\n",
      "69002\n",
      "69003\n",
      "69004\n",
      "69005\n",
      "69006\n",
      "69007\n",
      "69008\n",
      "69009\n",
      "69010\n",
      "69011\n",
      "69012\n",
      "69013\n",
      "69014\n",
      "69015\n",
      "69016\n",
      "69017\n",
      "69018\n",
      "69019\n",
      "69020\n",
      "69021\n",
      "69022\n",
      "69023\n",
      "69024\n",
      "69025\n",
      "69026\n",
      "69027\n",
      "69028\n",
      "69029\n",
      "69030\n",
      "69031\n",
      "69032\n",
      "69033\n",
      "69034\n",
      "69035\n",
      "69036\n",
      "69037\n",
      "69038\n",
      "69039\n",
      "69040\n",
      "69041\n",
      "69042\n",
      "69043\n",
      "69044\n",
      "69045\n",
      "69046\n",
      "69047\n",
      "69048\n",
      "69049\n",
      "69050\n",
      "69051\n",
      "69052\n",
      "69053\n",
      "69054\n",
      "69055\n",
      "69056\n",
      "69057\n",
      "69058\n",
      "69059\n",
      "69060\n",
      "69061\n",
      "69062\n",
      "69063\n",
      "69064\n",
      "69065\n",
      "69066\n",
      "69067\n",
      "69068\n",
      "69069\n",
      "69070\n",
      "69071\n",
      "69072\n",
      "69073\n",
      "69074\n",
      "69075\n",
      "69076\n",
      "69077\n",
      "69078\n",
      "69079\n",
      "69080\n",
      "69081\n",
      "69082\n",
      "69083\n",
      "69084\n",
      "69085\n",
      "69086\n",
      "69087\n",
      "69088\n",
      "69089\n",
      "69090\n",
      "69091\n",
      "69092\n",
      "69093\n",
      "69094\n",
      "69095\n",
      "69096\n",
      "69097\n",
      "69098\n",
      "69099\n",
      "69100\n",
      "69101\n",
      "69102\n",
      "69103\n",
      "69104\n",
      "69105\n",
      "69106\n",
      "69107\n",
      "69108\n",
      "69109\n",
      "69110\n",
      "69111\n",
      "69112\n",
      "69113\n",
      "69114\n",
      "69115\n",
      "69116\n",
      "69117\n",
      "69118\n",
      "69119\n",
      "69120\n",
      "69121\n",
      "69122\n",
      "69123\n",
      "69124\n",
      "69125\n",
      "69126\n",
      "69127\n",
      "69128\n",
      "69129\n",
      "69130\n",
      "69131\n",
      "69132\n",
      "69133\n",
      "69134\n",
      "69135\n",
      "69136\n",
      "69137\n",
      "69138\n",
      "69139\n",
      "69140\n",
      "69141\n",
      "69142\n",
      "69143\n",
      "69144\n",
      "69145\n",
      "69146\n",
      "69147\n",
      "69148\n",
      "69149\n",
      "69150\n",
      "69151\n",
      "69152\n",
      "69153\n",
      "69154\n",
      "69155\n",
      "69156\n",
      "69157\n",
      "69158\n",
      "69159\n",
      "69160\n",
      "69161\n",
      "69162\n",
      "69163\n",
      "69164\n",
      "69165\n",
      "69166\n",
      "69167\n",
      "69168\n",
      "69169\n",
      "69170\n",
      "69171\n",
      "69172\n",
      "69173\n",
      "69174\n",
      "69175\n",
      "69176\n",
      "69177\n",
      "69178\n",
      "69179\n",
      "69180\n",
      "69181\n",
      "69182\n",
      "69183\n",
      "69184\n",
      "69185\n",
      "69186\n",
      "69187\n",
      "69188\n",
      "69189\n",
      "69190\n",
      "69191\n",
      "69192\n",
      "69193\n",
      "69194\n",
      "69195\n",
      "69196\n",
      "69197\n",
      "69198\n",
      "69199\n",
      "69200\n",
      "69201\n",
      "69202\n",
      "69203\n",
      "69204\n",
      "69205\n",
      "69206\n",
      "69207\n",
      "69208\n",
      "69209\n",
      "69210\n",
      "69211\n",
      "69212\n",
      "69213\n",
      "69214\n",
      "69215\n",
      "69216\n",
      "69217\n",
      "69218\n",
      "69219\n",
      "69220\n",
      "69221\n",
      "69222\n",
      "69223\n",
      "69224\n",
      "69225\n",
      "69226\n",
      "69227\n",
      "69228\n",
      "69229\n",
      "69230\n",
      "69231\n",
      "69232\n",
      "69233\n",
      "69234\n",
      "69235\n",
      "69236\n",
      "69237\n",
      "69238\n",
      "69239\n",
      "69240\n",
      "69241\n",
      "69242\n",
      "69243\n",
      "69244\n",
      "69245\n",
      "69246\n",
      "69247\n",
      "69248\n",
      "69249\n",
      "69250\n",
      "69251\n",
      "69252\n",
      "69253\n",
      "69254\n",
      "69255\n",
      "69256\n",
      "69257\n",
      "69258\n",
      "69259\n",
      "69260\n",
      "69261\n",
      "69262\n",
      "69263\n",
      "69264\n",
      "69265\n",
      "69266\n",
      "69267\n",
      "69268\n",
      "69269\n",
      "69270\n",
      "69271\n",
      "69272\n",
      "69273\n",
      "69274\n",
      "69275\n",
      "69276\n",
      "69277\n",
      "69278\n",
      "69279\n",
      "69280\n",
      "69281\n",
      "69282\n",
      "69283\n",
      "69284\n",
      "69285\n",
      "69286\n",
      "69287\n",
      "69288\n",
      "69289\n",
      "69290\n",
      "69291\n",
      "69292\n",
      "69293\n",
      "69294\n",
      "69295\n",
      "69296\n",
      "69297\n",
      "69298\n",
      "69299\n",
      "69300\n",
      "69301\n",
      "69302\n",
      "69303\n",
      "69304\n",
      "69305\n",
      "69306\n",
      "69307\n",
      "69308\n",
      "69309\n",
      "69310\n",
      "69311\n",
      "69312\n",
      "69313\n",
      "69314\n",
      "69315\n",
      "69316\n",
      "69317\n",
      "69318\n",
      "69319\n",
      "69320\n",
      "69321\n",
      "69322\n",
      "69323\n",
      "69324\n",
      "69325\n",
      "69326\n",
      "69327\n",
      "69328\n",
      "69329\n",
      "69330\n",
      "69331\n",
      "69332\n",
      "69333\n",
      "69334\n",
      "69335\n",
      "69336\n",
      "69337\n",
      "69338\n",
      "69339\n",
      "69340\n",
      "69341\n",
      "69342\n",
      "69343\n",
      "69344\n",
      "69345\n",
      "69346\n",
      "69347\n",
      "69348\n",
      "69349\n",
      "69350\n",
      "69351\n",
      "69352\n",
      "69353\n",
      "69354\n",
      "69355\n",
      "69356\n",
      "69357\n",
      "69358\n",
      "69359\n",
      "69360\n",
      "69361\n",
      "69362\n",
      "69363\n",
      "69364\n",
      "69365\n",
      "69366\n",
      "69367\n",
      "69368\n",
      "69369\n",
      "69370\n",
      "69371\n",
      "69372\n",
      "69373\n",
      "69374\n",
      "69375\n",
      "69376\n",
      "69377\n",
      "69378\n",
      "69379\n",
      "69380\n",
      "69381\n",
      "69382\n",
      "69383\n",
      "69384\n",
      "69385\n",
      "69386\n",
      "69387\n",
      "69388\n",
      "69389\n",
      "69390\n",
      "69391\n",
      "69392\n",
      "69393\n",
      "69394\n",
      "69395\n",
      "69396\n",
      "69397\n",
      "69398\n",
      "69399\n",
      "69400\n",
      "69401\n",
      "69402\n",
      "69403\n",
      "69404\n",
      "69405\n",
      "69406\n",
      "69407\n",
      "69408\n",
      "69409\n",
      "69410\n",
      "69411\n",
      "69412\n",
      "69413\n",
      "69414\n",
      "69415\n",
      "69416\n",
      "69417\n",
      "69418\n",
      "69419\n",
      "69420\n",
      "69421\n",
      "69422\n",
      "69423\n",
      "69424\n",
      "69425\n",
      "69426\n",
      "69427\n",
      "69428\n",
      "69429\n",
      "69430\n",
      "69431\n",
      "69432\n",
      "69433\n",
      "69434\n",
      "69435\n",
      "69436\n",
      "69437\n",
      "69438\n",
      "69439\n",
      "69440\n",
      "69441\n",
      "69442\n",
      "69443\n",
      "69444\n",
      "69445\n",
      "69446\n",
      "69447\n",
      "69448\n",
      "69449\n",
      "69450\n",
      "69451\n",
      "69452\n",
      "69453\n",
      "69454\n",
      "69455\n",
      "69456\n",
      "69457\n",
      "69458\n",
      "69459\n",
      "69460\n",
      "69461\n",
      "69462\n",
      "69463\n",
      "69464\n",
      "69465\n",
      "69466\n",
      "69467\n",
      "69468\n",
      "69469\n",
      "69470\n",
      "69471\n",
      "69472\n",
      "69473\n",
      "69474\n",
      "69475\n",
      "69476\n",
      "69477\n",
      "69478\n",
      "69479\n",
      "69480\n",
      "69481\n",
      "69482\n",
      "69483\n",
      "69484\n",
      "69485\n",
      "69486\n",
      "69487\n",
      "69488\n",
      "69489\n",
      "69490\n",
      "69491\n",
      "69492\n",
      "69493\n",
      "69494\n",
      "69495\n",
      "69496\n",
      "69497\n",
      "69498\n",
      "69499\n",
      "69500\n",
      "69501\n",
      "69502\n",
      "69503\n",
      "69504\n",
      "69505\n",
      "69506\n",
      "69507\n",
      "69508\n",
      "69509\n",
      "69510\n",
      "69511\n",
      "69512\n",
      "69513\n",
      "69514\n",
      "69515\n",
      "69516\n",
      "69517\n",
      "69518\n",
      "69519\n",
      "69520\n",
      "69521\n",
      "69522\n",
      "69523\n",
      "69524\n",
      "69525\n",
      "69526\n",
      "69527\n",
      "69528\n",
      "69529\n",
      "69530\n",
      "69531\n",
      "69532\n",
      "69533\n",
      "69534\n",
      "69535\n",
      "69536\n",
      "69537\n",
      "69538\n",
      "69539\n",
      "69540\n",
      "69541\n",
      "69542\n",
      "69543\n",
      "69544\n",
      "69545\n",
      "69546\n",
      "69547\n",
      "69548\n",
      "69549\n",
      "69550\n",
      "69551\n",
      "69552\n",
      "69553\n",
      "69554\n",
      "69555\n",
      "69556\n",
      "69557\n",
      "69558\n",
      "69559\n",
      "69560\n",
      "69561\n",
      "69562\n",
      "69563\n",
      "69564\n",
      "69565\n",
      "69566\n",
      "69567\n",
      "69568\n",
      "69569\n",
      "69570\n",
      "69571\n",
      "69572\n",
      "69573\n",
      "69574\n",
      "69575\n",
      "69576\n",
      "69577\n",
      "69578\n",
      "69579\n",
      "69580\n",
      "69581\n",
      "69582\n",
      "69583\n",
      "69584\n",
      "69585\n",
      "69586\n",
      "69587\n",
      "69588\n",
      "69589\n",
      "69590\n",
      "69591\n",
      "69592\n",
      "69593\n",
      "69594\n",
      "69595\n",
      "69596\n",
      "69597\n",
      "69598\n",
      "69599\n",
      "69600\n",
      "69601\n",
      "69602\n",
      "69603\n",
      "69604\n",
      "69605\n",
      "69606\n",
      "69607\n",
      "69608\n",
      "69609\n",
      "69610\n",
      "69611\n",
      "69612\n",
      "69613\n",
      "69614\n",
      "69615\n",
      "69616\n",
      "69617\n",
      "69618\n",
      "69619\n",
      "69620\n",
      "69621\n",
      "69622\n",
      "69623\n",
      "69624\n",
      "69625\n",
      "69626\n",
      "69627\n",
      "69628\n",
      "69629\n",
      "69630\n",
      "69631\n",
      "69632\n",
      "69633\n",
      "69634\n",
      "69635\n",
      "69636\n",
      "69637\n",
      "69638\n",
      "69639\n",
      "69640\n",
      "69641\n",
      "69642\n",
      "69643\n",
      "69644\n",
      "69645\n",
      "69646\n",
      "69647\n",
      "69648\n",
      "69649\n",
      "69650\n",
      "69651\n",
      "69652\n",
      "69653\n",
      "69654\n",
      "69655\n",
      "69656\n",
      "69657\n",
      "69658\n",
      "69659\n",
      "69660\n",
      "69661\n",
      "69662\n",
      "69663\n",
      "69664\n",
      "69665\n",
      "69666\n",
      "69667\n",
      "69668\n",
      "69669\n",
      "69670\n",
      "69671\n",
      "69672\n",
      "69673\n",
      "69674\n",
      "69675\n",
      "69676\n",
      "69677\n",
      "69678\n",
      "69679\n",
      "69680\n",
      "69681\n",
      "69682\n",
      "69683\n",
      "69684\n",
      "69685\n",
      "69686\n",
      "69687\n",
      "69688\n",
      "69689\n",
      "69690\n",
      "69691\n",
      "69692\n",
      "69693\n",
      "69694\n",
      "69695\n",
      "69696\n",
      "69697\n",
      "69698\n",
      "69699\n",
      "69700\n",
      "69701\n",
      "69702\n",
      "69703\n",
      "69704\n",
      "69705\n",
      "69706\n",
      "69707\n",
      "69708\n",
      "69709\n",
      "69710\n",
      "69711\n",
      "69712\n",
      "69713\n",
      "69714\n",
      "69715\n",
      "69716\n",
      "69717\n",
      "69718\n",
      "69719\n",
      "69720\n",
      "69721\n",
      "69722\n",
      "69723\n",
      "69724\n",
      "69725\n",
      "69726\n",
      "69727\n",
      "69728\n",
      "69729\n",
      "69730\n",
      "69731\n",
      "69732\n",
      "69733\n",
      "69734\n",
      "69735\n",
      "69736\n",
      "69737\n",
      "69738\n",
      "69739\n",
      "69740\n",
      "69741\n",
      "69742\n",
      "69743\n",
      "69744\n",
      "69745\n",
      "69746\n",
      "69747\n",
      "69748\n",
      "69749\n",
      "69750\n",
      "69751\n",
      "69752\n",
      "69753\n",
      "69754\n",
      "69755\n",
      "69756\n",
      "69757\n",
      "69758\n",
      "69759\n",
      "69760\n",
      "69761\n",
      "69762\n",
      "69763\n",
      "69764\n",
      "69765\n",
      "69766\n",
      "69767\n",
      "69768\n",
      "69769\n",
      "69770\n",
      "69771\n",
      "69772\n",
      "69773\n",
      "69774\n",
      "69775\n",
      "69776\n",
      "69777\n",
      "69778\n",
      "69779\n",
      "69780\n",
      "69781\n",
      "69782\n",
      "69783\n",
      "69784\n",
      "69785\n",
      "69786\n",
      "69787\n",
      "69788\n",
      "69789\n",
      "69790\n",
      "69791\n",
      "69792\n",
      "69793\n",
      "69794\n",
      "69795\n",
      "69796\n",
      "69797\n",
      "69798\n",
      "69799\n",
      "69800\n",
      "69801\n",
      "69802\n",
      "69803\n",
      "69804\n",
      "69805\n",
      "69806\n",
      "69807\n",
      "69808\n",
      "69809\n",
      "69810\n",
      "69811\n",
      "69812\n",
      "69813\n",
      "69814\n",
      "69815\n",
      "69816\n",
      "69817\n",
      "69818\n",
      "69819\n",
      "69820\n",
      "69821\n",
      "69822\n",
      "69823\n",
      "69824\n",
      "69825\n",
      "69826\n",
      "69827\n",
      "69828\n",
      "69829\n",
      "69830\n",
      "69831\n",
      "69832\n",
      "69833\n",
      "69834\n",
      "69835\n",
      "69836\n",
      "69837\n",
      "69838\n",
      "69839\n",
      "69840\n",
      "69841\n",
      "69842\n",
      "69843\n",
      "69844\n",
      "69845\n",
      "69846\n",
      "69847\n",
      "69848\n",
      "69849\n",
      "69850\n",
      "69851\n",
      "69852\n",
      "69853\n",
      "69854\n",
      "69855\n",
      "69856\n",
      "69857\n",
      "69858\n",
      "69859\n",
      "69860\n",
      "69861\n",
      "69862\n",
      "69863\n",
      "69864\n",
      "69865\n",
      "69866\n",
      "69867\n",
      "69868\n",
      "69869\n",
      "69870\n",
      "69871\n",
      "69872\n",
      "69873\n",
      "69874\n",
      "69875\n",
      "69876\n",
      "69877\n",
      "69878\n",
      "69879\n",
      "69880\n",
      "69881\n",
      "69882\n",
      "69883\n",
      "69884\n",
      "69885\n",
      "69886\n",
      "69887\n",
      "69888\n",
      "69889\n",
      "69890\n",
      "69891\n",
      "69892\n",
      "69893\n",
      "69894\n",
      "69895\n",
      "69896\n",
      "69897\n",
      "69898\n",
      "69899\n",
      "69900\n",
      "69901\n",
      "69902\n",
      "69903\n",
      "69904\n",
      "69905\n",
      "69906\n",
      "69907\n",
      "69908\n",
      "69909\n",
      "69910\n",
      "69911\n",
      "69912\n",
      "69913\n",
      "69914\n",
      "69915\n",
      "69916\n",
      "69917\n",
      "69918\n",
      "69919\n",
      "69920\n",
      "69921\n",
      "69922\n",
      "69923\n",
      "69924\n",
      "69925\n",
      "69926\n",
      "69927\n",
      "69928\n",
      "69929\n",
      "69930\n",
      "69931\n",
      "69932\n",
      "69933\n",
      "69934\n",
      "69935\n",
      "69936\n",
      "69937\n",
      "69938\n",
      "69939\n",
      "69940\n",
      "69941\n",
      "69942\n",
      "69943\n",
      "69944\n",
      "69945\n",
      "69946\n",
      "69947\n",
      "69948\n",
      "69949\n",
      "69950\n",
      "69951\n",
      "69952\n",
      "69953\n",
      "69954\n",
      "69955\n",
      "69956\n",
      "69957\n",
      "69958\n",
      "69959\n",
      "69960\n",
      "69961\n",
      "69962\n",
      "69963\n",
      "69964\n",
      "69965\n",
      "69966\n",
      "69967\n",
      "69968\n",
      "69969\n",
      "69970\n",
      "69971\n",
      "69972\n",
      "69973\n",
      "69974\n",
      "69975\n",
      "69976\n",
      "69977\n",
      "69978\n",
      "69979\n",
      "69980\n",
      "69981\n",
      "69982\n",
      "69983\n",
      "69984\n",
      "69985\n",
      "69986\n",
      "69987\n",
      "69988\n",
      "69989\n",
      "69990\n",
      "69991\n",
      "69992\n",
      "69993\n",
      "69994\n",
      "69995\n",
      "69996\n",
      "69997\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "candidates = []\n",
    "sources =  [inputSentences[i] for i in range(len(inputSentences) - 70000 , len(inputSentences) -2)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    for i in range(len(sources)):\n",
    "\n",
    "      paraphrasing_sentence = sources[i]\n",
    "\n",
    "      paraphrasing_sentence = sentence_to_seq(paraphrasing_sentence, source_vocab_to_int)\n",
    "\n",
    "      paraphrasing_logits = sess.run(logits, {input_data: [paraphrasing_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(paraphrasing_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "      candidates.append(\" \".join([target_int_to_vocab[j] for j in paraphrasing_logits]))\n",
    "  \n",
    "      print(str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4mhrXy2tzHJi"
   },
   "outputs": [],
   "source": [
    "# generation of test dataset in order to calculate the evaluation metrics\n",
    "\n",
    "references =  [[outputSentences[i].split()] for i in range(len(outputSentences) - 70000 , len(outputSentences))]\n",
    "candidates = [candidates[i].split()[0:-1] for i in range(len(candidates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySovzbfnZIDK"
   },
   "outputs": [],
   "source": [
    "references =references[0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Az5SceUSzOaO"
   },
   "outputs": [],
   "source": [
    "#Save the test dataset\n",
    "\n",
    "phrases =open(\"/content/drive/My Drive/TenWithoutAtt/test_score_ref.txt\", \"a+\")\n",
    "paraphrases=open(\"/content/drive/My Drive/TenWithoutAtt/test_score_hyp.txt\", \"a+\")\n",
    "\n",
    "for i in range(len(candidates)):\n",
    "\n",
    "  phrases.write((' ').join(references[i][0])+'\\n')\n",
    "  paraphrases.write((' ').join(candidates[i])+'\\n')\n",
    "  \n",
    "phrases.close()\n",
    "paraphrases.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "PM762wVwzatg",
    "outputId": "143dc570-a0d1-4bfe-a8c4-2e77546580b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10506337943341278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#calculate corpus bleu\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "xG9vywu4zc9f",
    "outputId": "e596fc47-0f19-4bd1-e552-c89c2434f1cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006679368377155351\n"
     ]
    }
   ],
   "source": [
    "#calculate sentence bleu avg\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "sent_average_score= 0\n",
    "ref1, can1 = references, candidates\n",
    "for reff, hypp in zip(ref1, can1):\n",
    "  sent_average_score += sentence_bleu(reff, hypp)\n",
    "\n",
    "sent_average_score = sent_average_score / len(candidates)\n",
    "print(sent_average_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Without_Mecanism_English.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
